---
title       : Twitter-based Alerts of Major Disasters
subtitle    : Biomedical Big Data Seminar
author      : Rachael A. Callcut, MD, MSPH &amp; Sara E. Moore, MA
date        : 13 November 2017
output:
  xaringan::moon_reader:
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    css: ["default", "custom.css"]
    nature:
      beforeInit: "macros.js"
      highlightStyle: tomorrow-night-eighties # arta, ascetic, dark, default, far, github, googlecode, idea, ir_black, magula, monokai, rainbow, solarized-dark, solarized-light, sunburst, tomorrow, tomorrow-night-blue, tomorrow-night-bright, tomorrow-night, tomorrow-night-eighties, vs, zenburn
      highlightLines: true
      countIncrementalSlides: false
---

```{r knitr_setup, echo=FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.height = 6,
					  dpi = 300, fig.cap="", fig.align='center')
showtext::showtext_opts(dpi = 300)
```

```{r loadbib, echo=FALSE, cache=FALSE}
library(RefManageR)
bib = ReadBib("references.bib", check = FALSE)
BibOptions(check.entries = FALSE, style = "markdown",
		   cite.style = "authoryear", bib.style = "numeric")
```


# How to access these slides

### Via git:

```{bash, eval=FALSE}
git clone https://github.com/saraemoore/TwitterTraumaBBD2017.git
```

### Download directly:
https://github.com/saraemoore/TwitterTraumaBBD2017/archive/master.zip

### View online:
http://www.saraemoore.com/TwitterTraumaBBD2017

???

* I won't be showing all the code on the slides today, but it's all available in the R Markdown document used to create these slides in this repository on github, or in the corresponding R script that's also in the github repository.

---
class: inverse, fullscreen, middle, center

> "A mass casualty incident is defined as an event which generates more patients at one time than locally available resources can manage using routine procedures. It requires exceptional emergency arrangements and additional or extraordinary assistance." &mdash; .small[`r TextCite(bib, "whomci2007")`]

---

layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

<!--
  https://git.io/vFEx0
-->

---

# July 6, 2013

.left-column[
- Asiana Flight 214
- Boeing 777-200 ER
- Incheon, South Korea
- 307 persons aboard
    - 291 passengers
    - 16 crew
    - 70 Children

]

.right-column[

![:scale 60%](assets/img/image3.jpg)

![:scale 90%](assets/img/image4.jpg)
]

---

# "11:28 am Plane crash"

- 1st official airport notification: "Code 33: Plane down"
- 1st hospital notice: police officer in ED
- Confusing initial information:
    - 'small cargo plane'
    - 'large cargo plane'
    - 'commercial jet liner'
    - 'Everyone is okay'

.pull-left-wide[
![](assets/img/image5.png)
]

.pull-right-wide[
![](assets/img/sfocrash_after.jpg)
]

???

In the SF Asiana airplane crash, the vast majority of the early information from the event was erroneous. 'Disaster myths' are harmful and they can alter the way a community responds to an event. Although people do not intend to mislead, disasters are emotional, making perception of facts often skewed.

---

```{r fixcitation, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
TextCite(bib, "houstonetal2015", .opts = list(max.names = 1))
```

# What do we do?

- Watch the news?
- Check the internet?
- Wait for the 'official notification' to prepare?
- Don't worry about it?

> "Effective disaster communication may prevent a disaster or lessen its impact, whereas ineffective disaster communication may cause a disaster or make its effects worse." &mdash; .small[`r TextCite(bib, "houstonetal2015", .opts = list(max.names = 1))`]

???

Communications in times of mass casualty events, especially those involving natural disasters, have always posed challenges. In nearly every after action report, communication is cited as the most common entity needing improvements

---

# Instantaneous News

.pull-left-wide[
![:scale 98%](assets/img/instant_news2.jpg)
]

.pull-right-wide[
![:scale 98%](assets/img/instant_news1.jpg)
]



<!--

- 1st Twitter message/photo **30 seconds** after impact
- Retweeted 4,450 times in first 24 hours

    https://www.usatoday.com/story/news/nation/2013/07/07/social-media-asiana-airlines-tweets-david-eun-twitter-tweets/2496903/
-->

???

The difficulty for hospital based providers in the age of widespread availability of social media is trying to wade through the enormous amount of information that flows almost instantaneously following these events. This information nearly always precedes the activation of any formal notification process and we believe that social media as a form of non-traditional data has great potential to provide an opportunity to improve early warning of impending need to respond to mass or multiple casualty events. As an example, the county wide disaster page activation occurred nearly 30 minutes later then information appeared on social media feeds following the crash of a commercial jet liner in San Francisco with over 300 potentially injured occupants. In fact, the first tweet of a plane down occurred 30 seconds after the crash and preceded even the 911 calls. Drawing on the experience from the Asiana plane crash, we recognized the potential power of this modality for earlier hospital notification. Building on this conceptual idea, the present study is the first ever investigation of a prospective signally disaster preparedness activation mechanism built from these non-traditional data sources targeted at the hospital level.

---

# Modern Society's Digital Footprint

.pull-left[
- Every day, on average:
    - 269 billion emails are sent<sup>1</sup>
    - 1.37 billion people log on to Facebook<sup>2</sup>
    - over 500 million Tweets are posted<sup>3</sup>
    - over 500 million people log on to Instagram<sup>4</sup>
]

.pull-right[
![](assets/img/online_in_60s_2.jpg)
]

.footnote[1: [The Radicati Group, Feb 2017](http://www.radicati.com/wp/wp-content/uploads/2017/01/Email-Statistics-Report-2017-2021-Executive-Summary.pdf)<br />2: [Facebook, June 2017](https://newsroom.fb.com/company-info/)<br />3: [Twitter, 2016](https://business.twitter.com/en/basics.html)<br />4: [Instagram, 2017](https://instagram-press.com/our-story/)]

<!--
    https://www.allaccess.com/merge/archive/26034/what-your-audience-is-doing-when-they-re-not
-->

???

Social media plays an ever increasing role in our society with 69% of Americans using social media including approximately 78 million persons using it multiple times per day. (http://www.pewinternet.org/fact-sheet/social-media/)

Each minute it is estimated that 278,000 tweets are sent in the U.S. Social media is a highly interactive platform which is web based, allows two way communications between users, and occurs in a virtual space.

---
layout: false
class: inverse, fullscreen, middle, center

# The ability to detect the signal in the noise is the key...

---
layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

---

# Using Twitter to Track Events

.pull-left[
- Twitter &mdash; microblogging
    - two-way communication
- Used to track non-disaster public health events
]

.pull-right[
![](assets/img/journal.pntd.0005729.g001b.png)
]

![:scale 75%](assets/img/journal.pntd.0005729.g004.png)

.footnote[`r TextCite(bib, "twitterdengue2017", .opts = list(max.names = 1))`]

???

Twitter has also been described as a form of 'microblogging' that allows 'dissemination of information that is timely and vast' and it has become a highly utilized format by the public following worldwide mass casualty events.

Twitter allows two-way communication has been used to track non-disaster public health events, and we hypothesized that it could be used to develop a prospective multiple casualty warning signal for hospital based activation.

---

# Using Twitter to Track Disasters

.pull-left[
- Information delivery
    - 2009: Typhoon Morakot in Taiwan
    - 2010: Earthquake in Haiti
    - 2011: Egyptian uprising

- Fed agencies
    - FEMA
    - National Weather Service
    - US Government (2010)
]

.pull-right[
[![:scale 60%](assets/img/704px-Typhoon_Morakot_Aug_7_2009.jpg)](http://rapidfire.sci.gsfc.nasa.gov/gallery/?2009219-0807/Morakot.A2009219.0525.2km.jpg)
![](assets/img/embassy_sfocrash.png)
]

<!--
    `r TextCite(bib, "taiwantyphoon")`
    https://www.pbs.org/newshour/world/haiti-quake-propels-twitter-community-mapping-efforts
    https://www.wired.com/2010/01/texts-tweets-saving-haitians-from-the-rubble/

* `r TextCite(bib, "sewolferry2015")`
* `r TextCite(bib, "japanearthquake2014")`
* `r TextCite(bib, "spatiotemporal2014")`
* `r TextCite(bib, "bostonbombing2015")`
-->

???

Several prior case studies have been published detailing experiences with the platform ranging from directing rescuers to victims following typhoons, hurricanes, earthquakes, and terrorist events, to using the platform to exchange information about how to reach disaster response resources for recovery. To date, research that has been conducted on using these powerful tools during times of disaster are limited to epidemiologic reports of basic Twitter use statistics and characteristics of users.

Social media has played an integral role in several prior large scale disasters including the the 2011 Egyptian uprising [3], Taiwan typhoon in 2009 [4], and in the 2010 Haiti earthquake. In each of these cases, traditional communication methods were inadequate and the use of social media improved coordination efforts of disaster response and recovery. Similarly, social media has been successfully utilized in other fields to predict and respond to public health events.

Merchant RM, Elmer S, Lurie N. "Integrating Social Media into Emergency-Preparedness Efforts." NEJM. 365(4):289-291; 2011. https://doi.org/10.1056/NEJMp1103591 PMID: 21793742

Huang CM, Chan E, Hyder AA. "Web 2.0 and Internet Social Networking: A New tool for Disaster Management? - Lessons from Taiwan." BMC Medical Informatics & Decision Making. 10(57); 2010.

Advocates believe that social media tools have great promise for enhancing our current disaster management communication strategies but studies are lacking [3]. These concepts have remained largely theoretical and the limited prior data available have focused on the descriptions of the public use of social media following various worldwide crises. Frequently, delayed notification and lack of early information has hindered timely hospital based activations in large scale multiple casualty events. Although many institutions and federal agencies have incorporated a social media response to deliver information to the public following an event [3], there has been no focused research on leveraging this platform to develop improved and early disaster notification mechanisms to law enforcement, prehospital providers, or hospital systems.

---

# Objectives

Use real-time social media data to:

* detect mass-casualty incidents and/or
* predict location-specific trauma center volume.

Goal: to determine if Twitter feeds could be used to develop a prospective multiple casualty warning signal for hospital based activation.

???

Delayed notification and lack of early information hinder timely hospital based activations in large scale multiple casualty events. We hypothesized that Twitter real-time data would produce a unique and reproducible signal within minutes of multiple casualty events and we investigated the timing of the signal compared with other hospital disaster notification mechanisms.

The objective of this study was to 1) initiate the development of a qualitative model designed to provide an additional tool for hospital level disaster notification and 2) to assure that the model would not falsely activate in a control event.

---

# Methods

- Retrospective (historical Twitter data)
    - Five recent U.S. Multiple Casualty Incidents
    - Event initiation $\rightarrow$ 7 days post-event

- Prospective (Twitter data collected via streaming API)
    - Super Bowl 50 (San Francisco, Feb 2016)
    - Approximately 7 days prior to and 7 days after Super Bowl kickoff

???

Historical analysis to develop an early warning signal followed by data collection to prospectively 'test' the signal thresholds

---

# Acquiring Twitter data

* [Twitter's REST APIs](https://dev.twitter.com/rest/public)
    - Conduct singular searches, read user profile information, or post Tweets.
  - Most common: [Twitter Search API](https://dev.twitter.com/rest/public/search)
      - Limitations: [Rate limited](https://dev.twitter.com/rest/public/rate-limits) (max number of queries every 15 mins), max 100 tweets returned per query, only past 7 days, may not be complete
* [Twitter's Streaming API](https://dev.twitter.com/streaming/overview)
    - Monitor or process Tweets in real-time.
    - Most common: [Public streams](https://dev.twitter.com/streaming/public) &rarr; [POST statuses/filter](https://dev.twitter.com/streaming/reference/post/statuses/filter)
      - Returns public statuses that match one or more filter predicates.
      - Limitations: one connection at a time allowed per account, subject to streaming cap (max out at small percentage of total tweet volume), frequent reconnects may result in rate limiting/brief IP blocking

---

# Acquiring Twitter data, continued

* [Gnip](https://gnip.com/) (now owned by Twitter)
    - Costs &#36;&#36;&#36;
    - Realtime
        - Most common: [PowerTrack](https://gnip.com/realtime/powertrack/): filtered firehose
    - Historical
        - [Historical PowerTrack](http://support.gnip.com/code/historical_powertrack.html): RESTful version of PowerTrack
        - [30-Day Search API](https://gnip.com/historical/30-day-search/)
        - [Full Archive Search API](https://gnip.com/historical/full-archive-search/)

???

As of April, 2015, 'firehose' access is cut off for all third-party resellers. Now it all goes through Gnip, which was acquired by Twitter in May, 2014. ([1](http://www.forbes.com/sites/benkepes/2015/04/11/how-to-kill-your-ecosystem-twitter-pulls-an-evil-move-with-its-firehose/), [2](http://thenextweb.com/dd/2015/04/11/twitter-cuts-off-firehose-resellers-as-it-brings-data-access-fully-in-house/), [3](http://www.infoworld.com/article/2908869/big-data/twitters-firehose-shut-off-is-the-newest-hazard-of-the-api-economy.html))

---

# Processing Twitter data (in R)

.pull-left[

* Parsing JSON (javascript object notation) files:
  * [rjson](https://cran.r-project.org/web/packages/rjson/index.html),
  * [RJSONIO](https://cran.r-project.org/web/packages/RJSONIO/index.html), and
  * **[jsonlite](https://cran.r-project.org/web/packages/jsonlite/index.html)**.
* Reading from REST APIs:
    * **twitteR** ([CRAN](https://cran.r-project.org/web/packages/twitteR/index.html)/[github](https://github.com/geoffjentry/twitteR))
    * [RTwitterAPI](https://github.com/joyofdata/RTwitterAPI)
* Reading from Streaming API:
    * **streamR** ([CRAN](https://cran.r-project.org/web/packages/streamR/index.html)/[github](https://github.com/pablobarbera/streamR))
    * tweet2r ([CRAN](https://cran.r-project.org/web/packages/tweet2r/index.html)/[github](https://github.com/pauarago/tweet2r))
]

.pull-right[

* Authenticating via OAuth:
    * ROAuth ([CRAN](https://cran.r-project.org/web/packages/ROAuth/index.html)/[github](https://github.com/geoffjentry/ROAuth))
* Other Resources:
   * Workshop materials and slides: [Analyzing and Collecting Social Media Data with R](https://github.com/pablobarbera/social-media-workshop)
    * CRAN's ["Web Technologies" package collection](https://cran.r-project.org/web/views/WebTechnologies.html), particularly the "parsing Data from the Web" and "social media" sections
]

- Not in R?: See Twitter's list of tools for [other programming languages](https://dev.twitter.com/overview/api/twitter-libraries)

---
layout: false
class: inverse, fullscreen, middle, center

# Exploratory Analysis of Historical Twitter data

---
layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

---

# Data on MCIs

- Sandy Hook Elementary School shooting (**SH**)
    - 14 Dec 2012 @ 9:35:00 EST ([time of first 911 calls](http://cspsandyhookreport.ct.gov/cspshr/Reports/CFS_1200704559.zip))
    - $n \approx 1.8$ mil
- Boston Marathon bombing (**BB**)
    - 15 Apr 2013 @ 14:49:00 EDT ([detonation time of first bomb](http://www.mass.gov/eopss/docs/mema/after-action-report-for-the-response-to-the-2013-boston-marathon-bombings.pdf))
    - $n \approx 1.1$ mil
- Asiana Airlines 214 crash at SFO (**SF**)
    - 6 Jul 2013 @ 11:28:00 PDT ([seawall impact time](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR1401.pdf))
    - $n \approx 400,000$
- South Napa earthquake (**NE**)
    - 24 Aug 2014 @ 03:20:44 PDT ([earthquake origin time](http://earthquake.usgs.gov/earthquakes/eventpage/nc72282711))
    - $n \approx 200,000$
- Marysville Pilchuck High School shooting (**MV**)
    - 24 Oct 2014 @ 10:39:00 PDT ([time of first 911 call](https://everettwa.gov/DocumentCenter/View/3915))
    - $n \approx 250,000$

???

Observations summarized here include only those with a `postedTime` after the event time and prior to the end of the collection period (typically 7 days after the start of the collection period). When more than 20 levels of a factor exist, only the first 20 are displayed. Counts of missing values are excluded from graphical summaries. one row corresponds to one tweet (or retweet). Boxplots are displayed for continuous variables, with a green cross denoting the mean of that variable.

Using disaster specific search terms, all relevant tweets from the event to 7 days post-event were analyzed for 5 recent US based multiple casualty events (Boston Bombing [BB], SF Plane Crash [SF], Napa Earthquake [NE], Sandy Hook [SH], and Marysville Shooting [MV]). Quantitative and qualitative analysis of tweet utilization were compared across events.

Tweets relating to five recent U.S. multiple casualty events were acquired post hoc from Twitter via Gnip's Historical PowerTrack (http://support.gnip.com/apis/historical_api/). All data was purchased from the Twitter Corporation. The tweet content including the text of the tweet, user information, geocoding of the tweet, structure of the tweet, number of each user's followers, retweet counts, and timing of the tweets were supplied. The five multiple casualty events of interest included the Boston Bombing, San Francisco Asiana 214 Plane Crash, Napa Earthquake, Sandy Hook Elementary School shooting, and Marysville School Shooting. Relevant tweets posted on the day of the event and in the six days thereafter (for seven days total per event) were identified using the incident time of each event (as displayed here).

Historical Power Track. Accessed November 2015. http://support.gnip.com/apis/historical_api/

---

# Developing the Signal

- Data acquired post hoc from Twitter via Gnip's Historical PowerTrack
    - data was *purchased* from the Twitter Corp.
- 100% capture based on keywords
- Complete tweet content: tweet text, user information, geocoding, structure, # of user's followers, retweets, & timing

---

# Keywords for multiple casualty events

.smaller[

Disaster Event                                | Search Terms
--------------------------------------------- | ---------------------------------------------
Sandy Hook Elementary School shooting (SH)    | School (gunman OR 'Sandy Hook' OR victims OR shooting OR Newtown)
                                              | Newtown (shooting OR gunman OR student OR Sandy Hook OR elementary)
Boston Marathon bombing (BB)                  | Bomb (Boston OR marathon OR explosion OR terrorist OR finish line)
                                              | Boston (explosion OR terrorist)
Asiana Airlines crash (SF)                    | Plane (crash OR SFO OR runway OR San Francisco OR Asiana OR '214')
                                              | Asiana (Crash OR '214' OR Runway OR SFO OR San Francisco)
South Napa earthquake (NE)                    | Earthquake (San Francisco OR 'sf' OR Napa OR '6.0')
Marysville Pilchuck High School shooting (MV) | School (gunman OR Marysville OR victims OR shooting OR Washington)
                                              | Marysville (shooting OR gunman OR student)
]

???

Tweets were identified using disaster-specific filtering rules. Where more than one text pattern was listed for a given event, a tweet was included if it matched on either (or both). Filtering was performed via a case-insensitive keyword search where individual keywords were separated by boolean operators. A space denotes an 'and' operator, an uppercase 'OR' denotes an 'or' operator, and this logic is combined using parentheses to group clauses. Quotation marks indicate terms which should be matched exactly (not tokenized) (http://support.gnip.com/apis/powertrack/rules.html).

Tweet data was packaged as a collection of compressed JSON files, each containing tweets from one 10 minute span of a single event's seven day collection period. Data within these JSON files was imported and cleaned using R with package jsonlite and subsequently summarized and visualized using ggplot2. Any tweets posted prior to their respective event's time were excluded from the analytic sample.

---

# Analysis

- Mostly qualitative, with objective of identifying similarities across events
- Differing tweet per minute thresholds were compared to:
    - County disaster notification time (if a county disaster page was activated)
    - Hospital stand by notification times (if no county disaster page was performed)
    - Patient arrival times (obtained from after action reports).

???

Quantitative and qualitative analysis were performed to identify similarities across events and the timing of differing tweet per minute thresholds were compared to the county disaster signal for each event. Numerical summaries of interest were computed, including mean followers per user, proportion of collected tweets that were retweets, and maximum tweet-per-minute volume. To better structure visualizations for qualitative identification of common event 'signatures,' tweets meeting all criteria described above were summarized into tweets per minute during the first twelve hours after the event onset. The speed with which tweet volume reached a threshold and/or peak was compared to the timing of the traditional county disaster signal for each event. For multiple casualty events that require the resources beyond normal functioning, county governments in conjunction with local official coordinate disaster wide responses involving multiple jurisdictions and hospital systems. Each of the events utilized in this report generated a detailed after action report that produced by the responsible governing body. These reports can include detailed information on timing of the initial event, first EMS calls, hospital standby notification, and county wide disaster activation (if activated). Not all 'after action' reports report all metrics. These after action reports are considered the official documentation of the time line of the events and therefore, we utilized the metrics available in each report. To compare the speed with which tweets accumulated across events, the cumulative tweet volume in the first 60 minutes after each event's onset was also computed.

---

# Results

```{r twittereventmetadata, message=FALSE, echo=FALSE, warning=FALSE}
# http://www.mass.gov/eopss/docs/mema/after-action-report-for-the-response-to-the-2013-boston-marathon-bombings.pdf
# "Monday, April 15, 2013: 2:49pm First bomb detonates. Second bomb detonates 13 seconds later."
# https://en.wikipedia.org/wiki/Marysville_Pilchuck_High_School_shooting
# 
# http://earthquake.usgs.gov/earthquakes/eventpage/nc72282711#general_summary
# "2014-08-24 10:20:44 (UTC)"
# http://cspsandyhookreport.ct.gov/
# https://en.wikipedia.org/wiki/Sandy_Hook_Elementary_School_shooting
# 
# http://www.ntsb.gov/news/events/Pages/2014_Asiana_BMG-Abstract.aspx
# "On July 6, 2013, about 1128 Pacific daylight time"

# see also
# https://en.wikipedia.org/wiki/List_of_UTC_time_offsets

twitterEventInfo <- data.frame(shortName = c("boston", "marysville", "napa", "sandyhook", "sfo"),
  longName = c("Boston Marathon bombing", "Marysville shooting", "Napa earthquake", "Sandy Hook shooting", "SFO crash"),
  localTime = as.POSIXlt(c("2013-04-15 14:49:00-0400", # EDT
               "2014-10-24 10:39:00-0700", # PDT
               "2014-08-24 03:20:44-0700", # PDT
               "2012-12-14 9:35:00-0500", # EST
               "2013-07-06 11:28:00-0700"), #PDT
               format = "%Y-%m-%d %H:%M:%S%z",
               tz = "UTC"),
  shortTZ = c("EDT", "PDT", "PDT", "EST", "PDT"),
  scalesTZ = paste("America", c("New_York", "Los_Angeles", "Los_Angeles", "New_York", "Los_Angeles"), sep = "/"),
  twitterTZ = paste(c("Eastern", "Pacific", "Pacific", "Eastern", "Pacific"), "Time (US & Canada)"),
  stringsAsFactors = FALSE)
```

```{r loaddata_twitterevents, message=FALSE, echo=FALSE, warning=FALSE}
baseDir = file.path(getwd(), "..")

# using preprocessed data files to speed up rmd rendering
tweetsPerMinAll = readRDS(file.path(baseDir, "twitter_data", "event_tweets_per_min.rds"))
tweetMapDataAll = readRDS(file.path(baseDir, "twitter_data", "tweet_map_data.rds"))

# tweet.df = sapply(twitterEventInfo$shortName,
#     function(x) readRDS(file.path(baseDir, "twitter_data", paste(paste(x, "tweet_data", sep = "_"), "rds", sep = "."))),
#     simplify = FALSE)
# twitter_entities.hashtags.combo = sapply(twitterEventInfo$shortName,
#     function(x) readRDS(file.path(baseDir, "twitter_data", paste(paste(x, "hashtag_data", sep = "_"), "rds", sep = "."))),
#     simplify = FALSE)
```

- 3.8 million tweets
    - SH: 1,815,751
    - BB: 1,147,793
    - SF: 430,616
    - NE: 205,073
    - MV: 249,847
- 45% original tweets, 55% retweets
- Retweets mean of 82-564 times per event
- Mean followers 3382-9992 across events

???

Over 3.8 million tweets were analyzed (SH 1.8 m, BB 1.1m, SF 430k, MV 250k, NE 205k). Peak tweets per min ranged from 209-3326. The mean followers per tweeter ranged from 3382-9992 across events. Retweets were tweeted a mean of 82-564 times per event. Tweets occurred very rapidly for all events (<2 mins) and represented 1% of the total event specific tweets in a median of 13 minutes of the first 911 calls.

For the 5 recent U.S. multiple casualty events, over 3.8 million study tweets were analyzed including a per-event tweet count as shown. The distribution of tweets included unique tweets in 45% of postings and the remaining 55% were retweets or reposting of prior tweets. Retweets were tweeted a mean of 82-564 times per event. The mean followers of the person originating a tweet were ranged from 3382-9992 across events.

---

# Signal Threshold

- 200 tweets per minute?
- Initiated before patient arrival to local hospitals*
- Preceded traditional disaster notification and hospital standby notices in SF and NE, and followed within 2 mins in BB & MV

???

A 200 tweets/min threshold was reached fastest with NE (2 min), BB (7 min), and SF (18 mins). If this threshold was utilized as a signaling mechanism to place local hospitals on standby for possible large scale events, in all case studies, this signal would have preceded patient arrival. Importantly, this threshold for signaling would also have preceded traditional disaster notification mechanisms in SF, NE, and simultaneous with BB and MV.

All events reached their overall maximum peak tweet posting per minute rate within 3 hours following the incident with the peak rate being reached fastest in the NE, SF, and MV (Fig 1). The tweet graphic signatures were consistent across disasters except SH which had similar signature but, delayed signal initiation (Fig 1). Peak tweets per minute ranged from 209-3326 for each event with the median in the first 60 minutes from 10-2169 (BB: 2564; SF 549; MV 162; NE 368; SH 10). The total tweets in the first 60 minutes represented a high percentage of the total tweets in the 7 days following the incident time in all cases except SH (11.5% BB, 7.8% SF, 8.0% MV, 11.9% MV, 0.14% SH).

Initial tweets occurred very rapidly in all events with the first signals detected less than 2 mins after the incident start times. When isolating the analysis to only tweets in the first 60 minutes, 1% of the total event specific tweets were reached in a median of 13 minutes of the first 911 calls (Table 3) with the most rapid 1% threshold being reached in the NE (2 minutes), BB (9 minutes), and SF (13 minutes) (Table 3). The maximum tweets per minute in the first 60 minutes was highest for the BB (3326 tweets/minute), followed by SF (1423), MV (957), NE (739) and SH (209) (Fig 2).

Differing thresholds were compared to the county disaster notification time (if a county disaster page was activated), hospital stand-by notification times (if no county disaster page was performed), and patient arrival times (obtained from after action reports). A 200 tweet per minute threshold was reached fastest with NE (2 min), BB (7 min), and SF (18 mins). If this threshold was utilized as a signaling mechanism to place local hospitals on standby for possible large scale events, excluding SH, the signal would have preceded patient arrival to local hospitals in all cases. It would also have preceded traditional county disaster notification and hospital standby notices in SF and NE, and followed within minutes in BB (2.5 minutes).

In both school shootings, a clear county wide disaster activation time was not reported in the after action reports. In the case of MV, there was an all-county wide law enforcement communication sent 28 minutes after the incident. Using this as an indicator in place of an official disaster mass casualty warning, our signal threshold would have been initiated within 2 minutes of this notice and 16 minutes prior to patient arrival at the hospital. For the Sandy Hook (SH) school shooting time line there was no clear time for the county disaster page available or hospital stand by times available in official after action reports making analysis of our signal not possible. There were also no victims transported to hospitals and the first report of casualties for SH did not occur until approximately 40 minutes after the event incident time.


---
layout: false

class: fullscreen, middle, center

<!--
[Link](file:///Users/fpgcdi/Dropbox/Trauma\ and\ Coagulation\ \(White\ Space\ Conflict\)/Twitter/prelim_analysis/index.html)
-->

```{r twittertsplot_libsandfxns, message=FALSE, echo=FALSE, warning=FALSE}
library(lubridate)
library(ggplot2)
library(scales)
# library(grid)
# library(zoo)
# library(data.table)
# library(dplyr)
library(dygraphs)
library(xts)

theme_twitter = theme_minimal(base_size = 12) +
    theme(legend.position = "bottom",
      # plot.title = element_text(hjust = 0.5), # ggplot 2.2.0
      # plot.subtitle = element_text(hjust = 0.5), # ggplot 2.2.0
      # axis.text.x = element_text(angle = 90),
      axis.line = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.grid.major.y = element_line(colour = "white", size = 0.3),
          panel.grid.minor = element_blank(),
          strip.background = element_blank(),
      axis.text = element_text(margin = unit(-0.25, 'cm')),
          panel.ontop = TRUE)

theme_twitter_nolines = theme_twitter %+replace% theme(panel.grid.major.y = element_blank())

makeHLineAnnotation = function(x.interval, y, color, label = "max", alpha = 0.8,
                 linetype = 2, size = 3, hjust = -0.02, vjust = -0.7) {
  x = int_start(x.interval)
  list(
      geom_hline(yintercept = y,
           color = color,
           alpha = alpha,
           linetype = linetype),
      annotate("text", x = x,
             y = y,
             label = label,
             color = color,
                   alpha = alpha,
                   size = size,
                   hjust = hjust,
                   vjust = vjust)
  )
}

makeVLineAnnotation = function(x, y, label, color = "red",
                             alpha.label = 0.8, alpha.line = 0.4, size = 3.5,
                             angle = 90, hjust = 0.5, vjust = -0.15) {
  vannot = vector("list", 2)
  if(length(x) > 1) {
    vannot[[1]] = geom_polygon(data = data.frame(
        created_at = rep(x, each = 2),
        n = c(-Inf, Inf, Inf, -Inf)),
      fill = color, alpha = alpha.line)
  } else {
    vannot[[1]] = geom_vline(
      xintercept = as.numeric(x),
      color = color, alpha = alpha.line)
  }

  if(!is.null(label)) {
    vannot[[2]] = annotate("text", x = max(x),
        y = y, label = label,
        colour = color, alpha = alpha.label, size = size,
        vjust = vjust, hjust = hjust, angle = angle)
  } else {
    vannot = vannot[1]
  }
  return(vannot)
}

makeTwitterTSPlot = function(df, plot.time.interval, fill.under.line = TRUE,
               multi.line = FALSE, include.max.annot = TRUE,
                           event.label = NULL, event.time = NULL, adjust.y.max = 1.05,
                           event.label.y = mean(range(df$n)),
                           event.label.size = 3.5,
                           event.label.alpha = 0.8, event.line.alpha = 0.4,
                           event.label.color = "red", event.label.angle = 90,
                           event.label.vjust = -0.15, event.label.hjust = 0.5,
                           line.colors = c("gray20", "#5aae61"), axis.text.x.angle = 90,
                           plot.title = "Summary of tweets passing event AND location keyword filters",
                           plot.subtitle = "and where users self-reported Pacific time zone",
               subtitle.color = NULL, subtitle.align = 0.5, subtitle.size = rel(0.9),
               title.align = 0.5, breaks.x = "1 hours", label.format.x = "%e %b %l%P",
                           tz.to.use = "America/Los_Angeles") {

  p = ggplot(df, aes(x = with_tz(created_at, tz.to.use), y = n)) +
    scale_x_datetime("Posted time (local time zone)",
             timezone = tz.to.use,
             date_breaks = breaks.x,
             expand = c(0.01, 0),
             date_labels = label.format.x) +
    ggtitle(plot.title, subtitle = plot.subtitle)

  max.y.actual = max(df$n)
  if(fill.under.line) {
    p = p + theme_twitter +
            theme(plot.title = element_text(hjust = title.align),
                plot.subtitle = element_text(hjust = subtitle.align, size = subtitle.size),
                axis.text.x = element_text(angle = axis.text.x.angle)) +
            geom_bar(fill = line.colors[1],
                   width = 60, stat="identity")
  } else {
    p = p + theme_twitter_nolines +
            theme(plot.title = element_text(hjust = title.align),
                plot.subtitle = element_text(hjust = subtitle.align, size = subtitle.size),
                axis.text.x = element_text(angle = axis.text.x.angle))
    if(!is.null(subtitle.color)) {
      p = p + theme(plot.subtitle = element_text(colour = subtitle.color))
    }
    if(multi.line){
      p = p + geom_line(aes(color = count.type), size = 0.75) +
           scale_colour_manual("Tweet Count:", values = line.colors)
        if(include.max.annot) {
          other.levels = setdiff(levels(df$count.type), "Actual")
          for(i in seq_along(other.levels)) {
            p = p +
              makeHLineAnnotation(x.interval = plot.time.interval,
                              y = max(subset(df, count.type==other.levels[i])$n),
                              color = line.colors[i+1])
          }
        }
      max.y.actual = max(subset(df, count.type=="Actual")$n)
    } else {
      p = p + geom_line(size = 0.5, color = line.colors[1])
    }
  }

  if(include.max.annot) {
    p = p +
    makeHLineAnnotation(x.interval = plot.time.interval,
                    y = max.y.actual,
                    color = line.colors[1])
  }
  p = p +
    scale_y_continuous("Number of tweets per minute",
               limits = c(0, max.y.actual*adjust.y.max))

  if(!is.null(event.time)) {
    p = p + makeVLineAnnotation(x = event.time, y = event.label.y, label = event.label,
                            color = event.label.color,
                            alpha.label = event.label.alpha, alpha.line = event.line.alpha,
                  size = event.label.size, angle = event.label.angle,
                  hjust = event.label.hjust, vjust = event.label.vjust)
  }

  return(p)
}

# _tweets_per_min_line
makeTweetsPerMinLinePlot <- function(eventTweetsPerMin, eventMetadata) {
  eventPlus12hrs = interval(eventMetadata$localTime, eventMetadata$localTime + hours(12))

  makeTwitterTSPlot(subset(eventTweetsPerMin, created_at < int_end(eventPlus12hrs)),
      eventPlus12hrs,
      fill.under.line = FALSE,
      subtitle.color = "#FF6666", subtitle.align = 0, subtitle.size = rel(0.8),
      event.line.alpha = 0.6, event.time = int_start(eventPlus12hrs), adjust.y.max = 1.05,
      plot.title = "Summary of tweets passing filter(s) posted in first 12 hours after event",
      plot.subtitle = paste(eventMetadata$longName, "occurred at approx.",
                            format(lubridate::with_tz(int_start(eventPlus12hrs), tz = eventMetadata$scalesTZ),
                          "%l:%M%P on %e %b %Y", tz = eventMetadata$scalesTZ)),
      tz.to.use = eventMetadata$scalesTZ)
}
```


[
```{r tweetspermin_lineplot1, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
i = 1

makeTweetsPerMinLinePlot(tweetsPerMinAll[[twitterEventInfo[i, "shortName"]]],
                         twitterEventInfo[i,])
```
](tweetspermin_dygraph1.html)

```{r dygraph_tweetspermin1, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

i = 1
eventMetadata = twitterEventInfo[i,]
eventName = eventMetadata$shortName
tweets.per.min.xts = xts::xts(tweetsPerMinAll[[eventName]][,-which(colnames(tweetsPerMinAll[[eventName]])=="created_at"), drop = FALSE],
    order.by = tweetsPerMinAll[[eventName]][,"created_at"],
    tzone = eventMetadata$scalesTZ)

p = dygraph(tweets.per.min.xts,
    main = paste(eventMetadata$longName, "Tweets passing filter(s) after event", sep = ": "),
    ylab = "Number of tweets per minute") %>%
    dyAxis("x", drawGrid = FALSE,
        label = paste0("Posted time (", eventMetadata$shortTZ, " time zone)"),
        rangePad = 1) %>%
    dyRangeSelector(dateWindow = c(eventMetadata$localTime - minutes(10), eventMetadata$localTime + hours(12))) %>%
    dyHighlight(highlightSeriesOpts = list(strokeWidth = 1.5),
        highlightCircleSize = 3,
        highlightSeriesBackgroundAlpha = 0.8) %>%
    dyEvent(eventMetadata$localTime, "Event", labelLoc = "top") %>% # labelLoc = "bottom"
    dyOptions(includeZero = TRUE, useDataTimezone = TRUE,
        fillGraph = TRUE, fillAlpha = 0.3, colors = "#3B7EA1") %>% # founder's rock
    dyLegend(show = "always", hideOnMouseOut = FALSE)

htmlwidgets::saveWidget(p, "tweetspermin_dygraph1.html")
```

---
class: fullscreen, middle, center

[
```{r tweetspermin_lineplot2, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
i = 2

makeTweetsPerMinLinePlot(tweetsPerMinAll[[twitterEventInfo[i, "shortName"]]],
                         twitterEventInfo[i,])
```
](tweetspermin_dygraph2.html)

```{r dygraph_tweetspermin2, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

i = 2
eventMetadata = twitterEventInfo[i,]
eventName = eventMetadata$shortName
tweets.per.min.xts = xts::xts(tweetsPerMinAll[[eventName]][,-which(colnames(tweetsPerMinAll[[eventName]])=="created_at"), drop = FALSE],
    order.by = tweetsPerMinAll[[eventName]][,"created_at"],
    tzone = eventMetadata$scalesTZ)

p = dygraph(tweets.per.min.xts,
    main = paste(eventMetadata$longName, "Tweets passing filter(s) after event", sep = ": "),
    ylab = "Number of tweets per minute") %>%
    dyAxis("x", drawGrid = FALSE,
        label = paste0("Posted time (", eventMetadata$shortTZ, " time zone)"),
        rangePad = 1) %>%
    dyRangeSelector(dateWindow = c(eventMetadata$localTime - minutes(10), eventMetadata$localTime + hours(12))) %>%
    dyHighlight(highlightSeriesOpts = list(strokeWidth = 1.5),
        highlightCircleSize = 3,
        highlightSeriesBackgroundAlpha = 0.8) %>%
    dyEvent(eventMetadata$localTime, "Event", labelLoc = "top") %>% # labelLoc = "bottom"
    dyOptions(includeZero = TRUE, useDataTimezone = TRUE,
        fillGraph = TRUE, fillAlpha = 0.3, colors = "#3B7EA1") %>% # founder's rock
    dyLegend(show = "always", hideOnMouseOut = FALSE)

htmlwidgets::saveWidget(p, "tweetspermin_dygraph2.html")
```

---
class: fullscreen, middle, center

[
```{r tweetspermin_lineplot4, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
i = 4

makeTweetsPerMinLinePlot(tweetsPerMinAll[[twitterEventInfo[i, "shortName"]]],
                         twitterEventInfo[i,])
```
](tweetspermin_dygraph4.html)

```{r dygraph_tweetspermin4, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

i = 4
eventMetadata = twitterEventInfo[i,]
eventName = eventMetadata$shortName
tweets.per.min.xts = xts::xts(tweetsPerMinAll[[eventName]][,-which(colnames(tweetsPerMinAll[[eventName]])=="created_at"), drop = FALSE],
    order.by = tweetsPerMinAll[[eventName]][,"created_at"],
    tzone = eventMetadata$scalesTZ)

p = dygraph(tweets.per.min.xts,
    main = paste(eventMetadata$longName, "Tweets passing filter(s) after event", sep = ": "),
    ylab = "Number of tweets per minute") %>%
    dyAxis("x", drawGrid = FALSE,
        label = paste0("Posted time (", eventMetadata$shortTZ, " time zone)"),
        rangePad = 1) %>%
    dyRangeSelector(dateWindow = c(eventMetadata$localTime - minutes(10), eventMetadata$localTime + hours(12))) %>%
    dyHighlight(highlightSeriesOpts = list(strokeWidth = 1.5),
        highlightCircleSize = 3,
        highlightSeriesBackgroundAlpha = 0.8) %>%
    dyEvent(eventMetadata$localTime, "Event", labelLoc = "top") %>% # labelLoc = "bottom"
    dyOptions(includeZero = TRUE, useDataTimezone = TRUE,
        fillGraph = TRUE, fillAlpha = 0.3, colors = "#3B7EA1") %>% # founder's rock
    dyLegend(show = "always", hideOnMouseOut = FALSE)

htmlwidgets::saveWidget(p, "tweetspermin_dygraph4.html")
```

???

The tweet graphic signatures were consistent across disasters except SH which had similar signature but, delayed signal initiation.

---
class: fullscreen, middle, center

[
```{r tweetspermin_lineplot3, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
i = 3

makeTweetsPerMinLinePlot(tweetsPerMinAll[[twitterEventInfo[i, "shortName"]]],
                         twitterEventInfo[i,])
```
](tweetspermin_dygraph3.html)

```{r dygraph_tweetspermin3, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

i = 3
eventMetadata = twitterEventInfo[i,]
eventName = eventMetadata$shortName
tweets.per.min.xts = xts::xts(tweetsPerMinAll[[eventName]][,-which(colnames(tweetsPerMinAll[[eventName]])=="created_at"), drop = FALSE],
    order.by = tweetsPerMinAll[[eventName]][,"created_at"],
    tzone = eventMetadata$scalesTZ)

p = dygraph(tweets.per.min.xts,
    main = paste(eventMetadata$longName, "Tweets passing filter(s) after event", sep = ": "),
    ylab = "Number of tweets per minute") %>%
    dyAxis("x", drawGrid = FALSE,
        label = paste0("Posted time (", eventMetadata$shortTZ, " time zone)"),
        rangePad = 1) %>%
    dyRangeSelector(dateWindow = c(eventMetadata$localTime - minutes(10), eventMetadata$localTime + hours(12))) %>%
    dyHighlight(highlightSeriesOpts = list(strokeWidth = 1.5),
        highlightCircleSize = 3,
        highlightSeriesBackgroundAlpha = 0.8) %>%
    dyEvent(eventMetadata$localTime, "Event", labelLoc = "top") %>% # labelLoc = "bottom"
    dyOptions(includeZero = TRUE, useDataTimezone = TRUE,
        fillGraph = TRUE, fillAlpha = 0.3, colors = "#3B7EA1") %>% # founder's rock
    dyLegend(show = "always", hideOnMouseOut = FALSE)

htmlwidgets::saveWidget(p, "tweetspermin_dygraph3.html")
```

---
class: fullscreen, middle, center

[
```{r tweetspermin_lineplot5, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
i = 5

makeTweetsPerMinLinePlot(tweetsPerMinAll[[twitterEventInfo[i, "shortName"]]],
                         twitterEventInfo[i,])
```
](tweetspermin_dygraph5.html)

```{r sfo_dygraph_tweetspermin, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

eventName = "sfo"
eventMetadata = subset(twitterEventInfo, shortName==eventName)
tweets.per.min.xts = xts::xts(tweetsPerMinAll[[eventName]][,-which(colnames(tweetsPerMinAll[[eventName]])=="created_at"), drop = FALSE],
    order.by = tweetsPerMinAll[[eventName]][,"created_at"],
    tzone = eventMetadata$scalesTZ)

# library(highcharter)
# options(
#     highcharter.global = list(
#       # getTimezoneOffset = NULL,
#       # timezoneOffset = 0,
#       useUTC = FALSE
#       )
#     )
# highchart(type = "stock") %>% hc_add_series(tweets.per.min.xts, type = "line", name = "SFO crash") %>% hc_xAxis(type = "datetime") %>% hc_plotOptions(series = list(marker = list(enabled = FALSE)), line = list(dataGrouping = list(enabled = FALSE)))

p = dygraph(tweets.per.min.xts,
    main = paste(eventMetadata$longName, "Tweets passing filter(s) after event", sep = ": "),
    ylab = "Number of tweets per minute",
    width = 900, height = 600) %>% # default: width = 2400px, height = 1500px
    dyAxis("x", drawGrid = FALSE,
        label = paste0("Posted time (", eventMetadata$shortTZ, " time zone)"),
        rangePad = 1) %>%
    dyRangeSelector(dateWindow = c(eventMetadata$localTime - minutes(10), eventMetadata$localTime + hours(12))) %>%
    dyHighlight(highlightSeriesOpts = list(strokeWidth = 1.5),
        highlightCircleSize = 3,
        highlightSeriesBackgroundAlpha = 0.8) %>%
    dyEvent(eventMetadata$localTime, "Event", labelLoc = "top") %>% # labelLoc = "bottom"
    dyEvent(parse_date_time("July 6 2013 12:03", "b d Y H:M", tz = "America/Los_Angeles"),
        "Disaster page", labelLoc = "top") %>% # labelLoc = "bottom"
    dyOptions(includeZero = TRUE, useDataTimezone = TRUE,
        fillGraph = TRUE, fillAlpha = 0.3, colors = "#3B7EA1") %>% # founder's rock
    dyLegend(show = "always", hideOnMouseOut = FALSE)

htmlwidgets::saveWidget(p, "tweetspermin_dygraph5.html")

```


```{r tweetspermin_150mins_disasterpage_sfo, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}

eventName = "sfo"
eventMetadata = subset(twitterEventInfo, shortName==eventName)
sfoPlus150mins = interval(eventMetadata$localTime, eventMetadata$localTime + minutes(150))
p.sfo = makeTwitterTSPlot(subset(tweetsPerMinAll[[eventName]], created_at < int_end(sfoPlus150mins)),
    sfoPlus150mins,
    fill.under.line = FALSE, include.max.annot = FALSE,
        event.line.alpha = 0.6, event.label.vjust = 1.5,
        event.time = int_start(sfoPlus150mins),
        adjust.y.max = 1.05, event.label.y = max(tweetsPerMinAll[[eventName]]$n)*0.8,
        breaks.x = "30 mins", label.format.x = "%l:%M%P",
    plot.title = NULL, plot.subtitle = NULL,
    event.label = "SF Plane Crash",
        tz.to.use = eventMetadata$scalesTZ)

# Disaster page was at 12:03pm
p.sfo + makeVLineAnnotation(eventMetadata$localTime + minutes(33),
  max(tweetsPerMinAll[[eventName]]$n)*0.8,
  # mean(range(subset(tweetsPerMinAll[[eventName]], created_at < int_end(sfoPlus150mins))$n)),
  label = "Disaster Page", color = "darkgreen", vjust = 1.5, alpha.line = 0.6, alpha.label = 0.8)
```

---
class: fullscreen, middle, center

```{r tweetspermin_all_1hr_setup, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
event.tweets.per.min.1hr = vector("list", length(tweetsPerMinAll))
names(event.tweets.per.min.1hr) = names(tweetsPerMinAll)

for(i in names(tweetsPerMinAll)) {
  # event.plus.1hr = interval(subset(twitterEventInfo, shortName == i)$localTime,
                            # subset(twitterEventInfo, shortName == i)$localTime + minutes(60))
  event.tweets.per.min.1hr[[i]] = subset(tweetsPerMinAll[[i]],
                                         created_at < subset(twitterEventInfo, shortName == i)$localTime + minutes(60))
  event.tweets.per.min.1hr[[i]]$event = subset(twitterEventInfo, shortName == i)$longName
  event.tweets.per.min.1hr[[i]]$mins_since_event = difftime(event.tweets.per.min.1hr[[i]]$created_at,
                                lubridate::floor_date(subset(twitterEventInfo, shortName == i)$localTime, unit = "minute"),
                                    units = "mins")
  event.tweets.per.min.1hr[[i]]$cumuln = cumsum(event.tweets.per.min.1hr[[i]]$n)/sum(event.tweets.per.min.1hr[[i]]$n)
}
all.events.tweets.per.min.1hr = do.call(rbind, event.tweets.per.min.1hr)
all.events.tweets.per.min.1hr$event = as.factor(all.events.tweets.per.min.1hr$event)
all.events.tweets.per.min.1hr = all.events.tweets.per.min.1hr[-which(rownames(all.events.tweets.per.min.1hr) == "napa.61"),]

# added 20170217
all.events.tweets.per.min.1hr$event.short = as.character(all.events.tweets.per.min.1hr$event)
all.events.tweets.per.min.1hr[all.events.tweets.per.min.1hr$event=="Boston Marathon bombing", "event.short"] <- "BB"
all.events.tweets.per.min.1hr[all.events.tweets.per.min.1hr$event=="SFO crash", "event.short"] <- "SF"
all.events.tweets.per.min.1hr[all.events.tweets.per.min.1hr$event=="Marysville shooting", "event.short"] <- "MV"
all.events.tweets.per.min.1hr[all.events.tweets.per.min.1hr$event=="Napa earthquake", "event.short"] <- "NE"
all.events.tweets.per.min.1hr[all.events.tweets.per.min.1hr$event=="Sandy Hook shooting", "event.short"] <- "SH"
all.events.tweets.per.min.1hr$event.short = as.factor(all.events.tweets.per.min.1hr$event.short)
```

```{r volume_tweets_1hr_allevents, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
library(directlabels)
ggplot(all.events.tweets.per.min.1hr,
  aes(x = mins_since_event, y = n, color = event)) +
  geom_line(size = 0.75) +
  scale_x_continuous("Minutes Post-Event",
    breaks = seq(from = 0, to = 60, by = 5)) +
  scale_y_continuous("Tweets per minute",
    breaks = seq(from = 0, to = 3500, by = 500),
    limits = c(0, 3500)) +
  ggtitle("Tweet Volume in First 60 Minutes") +
  scale_colour_manual("", values = c("#332288", "#88CCEE", "#117733", "#DDCC77", "#CC6677")) +
  # scale_linetype_manual("", values = c("solid", "dashed", "dotted", "dotdash", "twodash"), guide = "none") +
  theme_twitter_nolines +
    theme(plot.title = element_text(hjust = 0.5)) +
  # theme_classic(base_size = 12) +
  #   theme(legend.position = "bottom",
  #       plot.title = element_text(hjust = 0.5)) +
  geom_dl(aes(label = event.short), method = "last.points") +
  guides(colour = guide_legend(override.aes = list(size=3), nrow = 2))
```

???

Tweets occurred very rapidly for all events (<2 mins)
1% of the total event specific tweets in a median of 13 minutes of the first 911 calls

---
class: fullscreen, middle, center

```{r cumulativepct_tweets_1hr_allevents, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}

ggplot(all.events.tweets.per.min.1hr,
  aes(x = mins_since_event, y = cumuln, color = event)) +
  geom_line(size = 0.75) +
  scale_x_continuous("Minutes Post-Event",
    breaks = seq(from = 0, to = 60, by = 5)) +
  scale_y_continuous("Cumulative percent", labels = scales::percent) +
  ggtitle("Cumulative Tweets in First 60 Minutes") +
  scale_colour_manual("", values = c("#332288", "#88CCEE", "#117733", "#DDCC77", "#CC6677")) +
  theme_twitter_nolines +
    theme(plot.title = element_text(hjust = 0.5)) +
  guides(colour = guide_legend(override.aes = list(size=3), nrow = 2))
```

---

# Hashtag use


---

layout: false
class: fullscreen, middle, center

```{r dt0, out.width='100%', out.height='100%', message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
library(DT)
dont_show_cols = which(grepl("^gho|^publish",
               colnames(who_cbdr)))
datatable(who_cbdr[,-dont_show_cols],
      caption = "Global population rates in 2013 per 1,000 population",
      rownames = FALSE,
      colnames = c("World Bank Income Group" = 2,
               "Crude birth rate" = 5,
               "Crude death rate" = 6),
      options = list(pageLength = 5,
               lengthMenu = c(5, 10, 15)))
```

---

# Localization

---

class: fullscreen, middle, center


```{r sfo_leafletplot_setup, message=FALSE, echo=FALSE, cache=TRUE, warning=FALSE}

eventName = "sfo"
tweet.map.df = tweetMapDataAll[[eventName]]

tweet.map.df$lat = tweet.map.df$geo.coordinates.latitude
tweet.map.df[is.na(tweet.map.df$geo.coordinates.latitude), "lat"] = tweet.map.df[is.na(tweet.map.df$geo.coordinates.latitude), "location.geo.coordinate.centroids.latitude"]

tweet.map.df$lng = tweet.map.df$geo.coordinates.longitude
tweet.map.df[is.na(tweet.map.df$geo.coordinates.longitude), "lng"] = tweet.map.df[is.na(tweet.map.df$geo.coordinates.longitude), "location.geo.coordinate.centroids.longitude"]
```

```{r sfo_leafletplot, fig.align='center', out.width='100%', out.height='100%', message=FALSE, echo=FALSE, cache=TRUE, warning=FALSE}
library(leaflet)

# "object.geo.coordinates.latitude", "object.geo.coordinates.longitude"
# "object.location.geo.coordinate.centroids.latitude", "object.location.geo.coordinate.centroids.longitude"

p = leaflet(tweet.map.df[,c("lat", "lng", "body", "postedTime", "object.link")]) %>% #addTiles() %>%
  addProviderTiles(providers$OpenStreetMap.Mapnik) %>%
  addMarkers(~lng, ~lat, popup = ~paste0(postedTime, ": ", body, " (", object.link, ")"),
    clusterOptions = markerClusterOptions())

htmlwidgets::saveWidget(p, "sfo_leaflet.html")
```

[![](assets/img/sfo_leaflet_screenshot.png)]](sfo_leaflet.html)

```{r sfo_ggmap, message=FALSE, echo=FALSE, eval=FALSE, warning=FALSE}
library(ggmap)

USAMap +
    geom_point(aes(x=lon, y=lat), data=mv_num_collisions, col="orange", alpha=0.4, size=mv_num_collisions$collisions*circle_scale_amt) +
    scale_size_continuous(range=range(mv_num_collisions$collisions))

```

---
layout: false
class: inverse, fullscreen, middle, center

# Data Visualization

---

layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

---

# Visualization

**[ggplot2](http://ggplot2.tidyverse.org)**
- [package source](https://github.com/tidyverse/ggplot2), [docs](http://ggplot2.tidyverse.org/reference/)

.pull-left[

**[htmlwidgets](http://www.htmlwidgets.org/)**: interactive data visualization framework in R
- provides the framework on which most of the interactive plotting packages in R rely (if they use JavaScript plotting libraries on the backend).
- [package source](https://github.com/ramnathv/htmlwidgets), [docs](http://www.htmlwidgets.org/), [gallery](http://gallery.htmlwidgets.org/)
- Advanced usage may require knowledge/use of JavaScript.
- Use with [R Markdown](http://rmarkdown.rstudio.com) or [Shiny](http://shiny.rstudio.com/)
]

.pull-right[
- [sparkline](https://github.com/htmlwidgets/sparkline): interactive versions of very small/inline graphics (Edward Tufte)
- [DT](https://github.com/rstudio/DT) ([docs](http://rstudio.github.io/DT/)): interactive tables (aka DataTables)
- [dygraphs](https://github.com/rstudio/dygraphs) ([docs](http://rstudio.github.io/dygraphs)): interactive time series plots
- [streamgraph](http://hrbrmstr.github.io/streamgraph/): interactive streamgraphs, also for time series
- [leaflet](https://github.com/rstudio/leaflet) ([docs](http://rstudio.github.io/leaflet)): interactive maps
* $\ldots$
]

---

# More `htmlwidgets` packages

.pull-left[
- [Highcharter](http://jkunst.com/highcharter/): interface to the Highcharts JavaScript library
- [formattable](https://renkun.me/formattable/): visualize/format vectors and data frames
- [threejs](https://github.com/bwlewis/rthreejs): interactive 3d plots and globes
- [MetricsGraphics](http://hrbrmstr.github.io/metricsgraphics/): interface to the MetricsGraphics D3 JavaScript library
- [rbokeh](https://github.com/bokeh/rbokeh) ([docs](http://hafen.github.io/rbokeh/), [useR2016 slides](http://slides.com/hafen/rbokeh#/)): interface to the Bokeh Python library
- [networkD3](http://christophergandrud.github.io/networkD3/): interactive [D3](http://d3js.org/) JavaScript network graphs
- [visNetwork](https://github.com/datastorm-open/visNetwork) ([docs](http://datastorm-open.github.io/visNetwork)): interactive network graphs
]

.pull-right[
- [plotly](https://github.com/ropensci/plotly) ([docs](https://plot.ly/r/)):
    - Convert `ggplot2` graphics $\rightarrow$ interactive graphics via `ggplotly()` and the plot.ly [ggplot2 library](https://plot.ly/ggplot2/) OR
    - Create interactive graphics directly with `plot_ly()` and the plot.ly [R library](https://plot.ly/r/)
    - Can operate entirely locally (your plot isn't automatically uploaded to plot.ly's servers and shared with the world).
- [ggiraph](https://github.com/davidgohel/ggiraph); [ggiraphExtra](https://github.com/cardiomoon/ggiraphExtra): make `ggplot2` graphics interactive.
- [d3heatmap](https://github.com/rstudio/d3heatmap) ([demo](http://rpubs.com/jcheng/mtcars-heatmap)): interactive heatmaps
]

---
layout: false
class: inverse, fullscreen, middle, center

# Comparison with Prospective Twitter data

---

layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

---

```{r superbowl_metadata, message=FALSE, echo=FALSE, warning=FALSE}
trackTermsLocation = c("sf", "sanfrancisco", "san", "francisco", "sanfran",
  "prayforsanfrancisco", "bayarea", "sfbayarea", "bay", "sanfranciscobay",
  "northerncalifornia", "norcal", "california", "ca",
  "sfgh", "zfgh", "thegeneral", "general", "goldengatebridge", "goldengate",
  "bridge", "baybridge", "bart", "caltrain", "muni", "cablecar")
trackTermsEvent = c("breaking", "news", "breakingnews", "cnn", "pray",
  "crash", "shot", "shooting", "stab", "stabbed", "stabbing", "fall",
  "dead", "died", "accident", "earthquake", "flood", "victim", "victims",
  "fatality", "fatalities", "attack")
```

```{r loaddata_superbowl, message=FALSE, echo=FALSE, warning=FALSE, cache=TRUE}
tweet_hist_data = readRDS(file.path(baseDir, "tweet_hist_data.rds"))
```

# Super Bowl 50

.pull-left[
- Match 1+ location keywords AND/OR 1+ event keywords
- 2 weeks (Sun, 31 Jan 2016 until Sun, 14 Feb 2016)
- Collected `r prettyNum(sum(tweet_hist_data[["unfilt"]]$n), big.mark = ",")` tweets, retweets, and quote tweets
    - +248 truncated records (discarded)
    - +6,675,076 records not captured (exceeded rate limit)
]

.pull-right[
.smaller[
- **'Event' keywords:** `r paste(trackTermsEvent, collapse = ", ")`
- **'Location' keywords:** `r paste(trackTermsLocation, collapse = ", ")`
]
]

???

Event & location key words utilized in the prospective test signal for Super Bowl 50.

A prospective API (R-based application program interface) was then constructed using generic search terms for potential multiple casualty events and was prospectively used to gather data from Twitter via their API (application programming interface) during a recent high profile sporting event (SF Super Bowl 50). SB50 was to be used as a control event to test the tweet per minute thresholds developed from the prior 5 US multiple casualty events. We compared the multiple casualty event results to the control event to determine how often a warning would have fired in error for various tweet/minute threshold signals using potential multiple casualty search terms or 'event' and a 'location' keyword. The key words were developed by studying both the most frequently used hashtags and keywords common across the five previously analyzed events ("breakingnews", "cnn", "pray", etc.) and by utilizing knowledge of locally relevant hashtags and keywords ("sf", "sfgh", "zfgh", "thegeneral", etc.).

From Sun, Jan 31, 2016 at 18:43:14 PST until Sun, Feb 14, 2016 at 16:03:49 PST, connected to Twitter's 'POST statuses/filter' public stream with the following 'track keywords'/'filter predicates', where a 'phrase' matched if *any* of the terms were present in the Tweet (order- and case-insensitive):

According to Twitter, "The text of the Tweet and some entity fields are considered for matches. Specifically, the `text` attribute of the Tweet, `expanded_url` and `display_url` for links and media, `text` for hashtags, and `screen_name` for user mentions are checked for matches."

Moving Average calculated

<!--
- No signal activation even during the height of the Super Bowl activities
-->

As a proof of concept exercise, a prospective API using generic search terms for the event was built in anticipation of the Super Bowl 50 to test the threshold of 200 tweets per minute for triggering notification of multiple casualty events. The goal was to test whether a high profile, frequently tweeted public event would falsely trigger the signal even if no mass casualty situation arose during the event. The API gathered relevant tweets that included one or more of the generic terms plus a location or event term for a two week period beginning one week before the Super Bowl and continuing for 7 days following the Super Bowl.

An estimated 55,355,870 original tweets, retweets, and quote tweets were collected over the approximately two week period. Not included in this total were 248 partial or truncated tweets which were discarded for incomplete data. The contents of an estimated 6,675,076 additional tweets were not captured because, at some moments, the provided keywords matched more tweets than Twitter's imposed rate limit allowed to be delivered.


Estimated total number of tweets (including retweets and quote tweets) collected: `r prettyNum(sum(tweet_hist_data[["unfilt"]]$n), big.mark = ",")` <!--55,355,870-->. This estimate takes into account the 248 partial/truncated tweets collected but discarded for incomplete data as well as an estimated 6,675,076 records which were not captured because the keywords matched more Tweets than Twitter's imposed rate limit allowed to be delivered.

The volume of Super Bowl tweets returned at any moment in time were subject to a "streaming cap": the free public filtered streams offered by Twitter "max out" at a small percentage (approximately 1%) of the total volume of tweets coming through Twitter's "firehose." When the streaming cap is exceeded, a rate limiting message is issued by the streaming service indicating how many tweets were missed. Tweets were collected into JSON files which were rotated approximately every half hour. JSON files were preprocessed via shell script and imported and cleaned utilizing the jsonlite and tidyjson packages in R. Tweets containing at least one event-related keyword AND at least one location-related keyword were summarized as tweets per minute over the entire collection period.

---
class: fullscreen, middle, center

```{r dygraphtweetsperminplot1, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
# Summary of tweets passing event OR location keyword filter in a two hour period, by minute
tweet_hist_data[["unfilt"]] = plyr::rename(tweet_hist_data[["unfilt"]], c("n" = "count"))
tweets.per.min.unfilt.xts = xts::xts(
    tweet_hist_data[["unfilt"]][,-which(colnames(tweet_hist_data[["unfilt"]])=="created_at"), drop = FALSE],
    order.by = tweet_hist_data[["unfilt"]][,"created_at"],
    tzone = "America/Los_Angeles")

dygraph(tweets.per.min.unfilt.xts,
    main = "Histogram of tweets passing event OR location keyword filter",
    ylab = "Number of tweets per minute") %>%
    dyAxis("x", drawGrid = FALSE,
        label = "Posted time (local time zone)", 
        rangePad = 1) %>%
    dyRangeSelector() %>%
    dyHighlight(highlightSeriesOpts = list(strokeWidth = 1.5),
        highlightCircleSize = 3, 
        highlightSeriesBackgroundAlpha = 0.8) %>%
    # dyEvent(strptime("2016-02-02 10:05:00-0700", "%Y-%m-%d %H:%M:%S%z", tz = "UTC"), "Event range start", labelLoc = "top") %>%
    dyEvent(parse_date_time("Feb 2 2016 10:05", "b d Y H:M", tz = "America/Los_Angeles"), "Event range", labelLoc = "bottom") %>%
    dyEvent(parse_date_time("Feb 2 2016 10:10", "b d Y H:M", tz = "America/Los_Angeles"), "Event range", labelLoc = "bottom") %>%
    dyShading(parse_date_time("Feb 2 2016 10:05", "b d Y H:M", tz = "America/Los_Angeles"),
        parse_date_time("Feb 2 2016 10:10", "b d Y H:M", tz = "America/Los_Angeles"),
        color = "#EFEFEF", axis = "x") %>%
    dyOptions(includeZero = TRUE, useDataTimezone = TRUE, fillGraph = TRUE, fillAlpha = 0.3) %>%
    dyLegend(show = "always", hideOnMouseOut = FALSE)

# saveWidget(widget = p, file="sb_tweets_per_min_unfiltered.html", selfcontained = FALSE)
```

---
class: fullscreen, middle, center

```{r sb_kickoff_tweets_per_min_setup, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
# super bowl: sun, feb 7 at 6:30pm ET

sb.kickoff.time = lubridate::ymd_hm("2016 Feb 7 15:30", tz = "America/Los_Angeles")
sb.kickoff.12hrs = interval(sb.kickoff.time - hours(12), sb.kickoff.time + hours(12))
```

```{r sb_kickoff_12hrs_tweets_per_min_filtered_userpactime, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% sb.kickoff.12hrs),
     sb.kickoff.12hrs,
         line.colors = "gray40",
         event.label = "Super Bowl kickoff occurred at\napprox. 3:30pm on 7 Feb",
         event.label.y = 300, event.time = sb.kickoff.time)
```

```{r sb_kickoff_12hrs_tweets_per_min_filtered_userpactime_line, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% sb.kickoff.12hrs),
     sb.kickoff.12hrs,
     fill.under.line = FALSE,
     event.label = "Super Bowl kickoff occurred at\napprox. 3:30pm on 7 Feb",
     event.label.y = 300, event.time = sb.kickoff.time)
```

---

# Suspicious spike


.pull-left[
[![:scale 90%](assets/img/tweetstorm_screenshot_20161219.jpg)](https://twitter.com/Centric510/status/696556613109329920)
]

.pull-right[
- Many retweets of a single tweet in rapid succession
    - 419 times in 29 seconds
    - Auto-retweets/bots?
- Lesson: ignore unsustained peaks
    - $\rightarrow$ smooth the time series
]

???

Screenshot captured on 19 Dec 2016

419 times were between 7 Feb 2016 9:05:30pm PST and 7 Feb 2016 9:05:59pm PST

---
class: fullscreen, middle, center

```{r sb_MA_setup, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
library(zoo)

midweek.time = lubridate::ymd_hm("2016 Feb 5 12:00", tz = "America/Los_Angeles")
midweek.12hrs = interval(midweek.time - hours(12), midweek.time + hours(12))

tweet_hist_data.bothfilt.userpactime.copy <- tweet_hist_data[["bothfilt.userpactime"]]
bothfilt.userpactime.zoo = zoo(tweet_hist_data.bothfilt.userpactime.copy$n, tweet_hist_data.bothfilt.userpactime.copy$created_at)

movavg.5mins = rollmean(bothfilt.userpactime.zoo, 5, fill = list(NA, NULL, NA))
movavg.60mins = rollmean(bothfilt.userpactime.zoo, 60, fill = list(NA, NULL, NA))

tweet_hist_data.bothfilt.userpactime.copy$n = coredata(movavg.5mins)
tweet_hist_data.bothfilt.userpactime_5minMA = rbind(tweet_hist_data[["bothfilt.userpactime"]], tweet_hist_data.bothfilt.userpactime.copy)
tweet_hist_data.bothfilt.userpactime_5minMA$count.type = as.factor(rep(c("Actual", "Moving Average (5 min window)"),
                                                              each = nrow(tweet_hist_data.bothfilt.userpactime.copy)))

tweet_hist_data.bothfilt.userpactime.copy$n = coredata(movavg.60mins)
tweet_hist_data.bothfilt.userpactime_60minMA = rbind(tweet_hist_data[["bothfilt.userpactime"]], tweet_hist_data.bothfilt.userpactime.copy)
tweet_hist_data.bothfilt.userpactime_60minMA$count.type = as.factor(rep(c("Actual", "Moving Average (60 min window)"),
                                                              each = nrow(tweet_hist_data.bothfilt.userpactime.copy)))
```

```{r sb_kickoff_12hrs_tweets_per_min_filtered_userpactime_line_withMA, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data.bothfilt.userpactime_5minMA, created_at %within% sb.kickoff.12hrs),
     sb.kickoff.12hrs,
     fill.under.line = FALSE,
     multi.line = TRUE,
     event.label = "Super Bowl kickoff occurred at\napprox. 3:30pm on 7 Feb",
     event.label.y = 300, event.time = sb.kickoff.time)
```

```{r sb_kickoff_12hrs_tweets_per_min_filtered_userpactime_line_withMA_notitle, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
# added 20170217
makeTwitterTSPlot(subset(tweet_hist_data.bothfilt.userpactime_5minMA, created_at %within% sb.kickoff.12hrs),
     sb.kickoff.12hrs,
     fill.under.line = FALSE,
     multi.line = TRUE,
     plot.title = NULL,
     plot.subtitle = NULL,
     event.label = "Super Bowl kickoff occurred at\napprox. 3:30pm on 7 Feb",
     event.label.y = 300, event.time = sb.kickoff.time)
```

```{r bkgdnoise_5feb_tweets_per_min_filtered_userpactime_line_with60minMA, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
# more smoothing to roughly quantify "background noise" on a day when no big events happened (Fri Feb 5)
makeTwitterTSPlot(subset(tweet_hist_data.bothfilt.userpactime_60minMA, created_at %within% midweek.12hrs),
     midweek.12hrs,
     fill.under.line = FALSE,
     multi.line = TRUE,
     include.max.annot = FALSE,
     event.label.y = 300,
     line.colors = c("gray20", "#e08214"))
```

---

# Unexpected Low Casualty Event

![](assets/img/chp_stabbing_sfgate.png)

???

http://www.sfgate.com/crime/article/CHP-Officer-attacked-and-injured-in-San-Francisco-6801249.php

Although not planned, while the streaming data collection was running, a high profile but low casualty count (one victim) event occurred during the Super Bowl near the Fan Zone Experience in San Francisco. This event provided an additional test to our signal to determine if a low casualty count but high profile trauma would falsely activate our signal. On Tuesday, February 2, 2016, between approximately 10:05 (time encounter began) and 10:10 (time of distress call) PST, a California Highway Patrol (CHP) officer was stabbed in San Francisco [16]. The first tweet about the stabbing from an SFPD officer appeared at 10:30 PST [17].

---
class: fullscreen, middle, center

```{r setupchpplots, message=FALSE, echo=FALSE, warning=FALSE}
chp.event.times = lubridate::ymd_hm(c("2016 Feb 2 10:05", "2016 Feb 2 10:10"), tz = "America/Los_Angeles")
chp.event.12hrs = interval(min(chp.event.times) - hours(12), max(chp.event.times) + hours(12))
chp.event.2hrs = interval(min(chp.event.times) - minutes(5), max(chp.event.times) + minutes(110))
# chp.10am.to.10am = interval(min(chp.event.times) - minutes(5) - hours(12),
#                           min(chp.event.times) - minutes(5) + hours(12))
```


```{r chp_12hrs_tweets_per_min_filtered_userpactime, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% chp.event.12hrs),
     chp.event.12hrs,
         event.label = "CHP stabbing occurred between\n10:05am and 10:10am on 2 Feb",
         event.time = chp.event.times,
         line.colors = "gray40",
         adjust.y.max = 1.1, event.label.y = 122, event.label.size = 3.25)
```

```{r chp_12hrs_tweets_per_min_filtered_userpactime_line, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% chp.event.12hrs),
     chp.event.12hrs,
     fill.under.line = FALSE,
     event.label = "CHP stabbing occurred between\n10:05am and 10:10am on 2 Feb",
     event.time = chp.event.times,
         adjust.y.max = 1.1, event.label.y = 122, event.label.size = 3.25)
```

```{r chp_12hrs_tweets_per_min_filtered_userpactime_line_notitle, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
# added 20170217
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% chp.event.12hrs),
     chp.event.12hrs,
     fill.under.line = FALSE,
     event.label = "CHP stabbing occurred between\n10:05am and 10:10am on 2 Feb",
     plot.title = NULL,
     plot.subtitle = NULL,
     event.time = chp.event.times,
         adjust.y.max = 1.1, event.label.y = 122, event.label.size = 3.25)
```

```{r chp_12hrs_tweets_per_min_filtered_userpactime_line_horizlabel, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% chp.event.12hrs),
     chp.event.12hrs, breaks.x = "2 hours",
     fill.under.line = FALSE, event.line.alpha = 0.6,
     subtitle.color = "#FF6666", subtitle.size = rel(0.8),
     plot.subtitle = "CHP stabbing occurred between 10:05am and 10:10am on 2 Feb",
     plot.title = "Summary of tweets passing event AND location keyword filters\nand where users self-reported Pacific time zone",
     event.time = chp.event.times)
```

class: fullscreen, middle, center

```{r chp_12hrs_tweets_per_min_filtered_userpactime_line_withMA, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data.bothfilt.userpactime_5minMA, created_at %within% chp.event.12hrs),
     chp.event.12hrs,
     fill.under.line = FALSE,
     multi.line = TRUE,
     event.label = "CHP stabbing occurred between\n10:05am and 10:10am on 2 Feb",
     event.time = chp.event.times,
         adjust.y.max = 1.1, event.label.y = 122, event.label.size = 2.5)
```

class: fullscreen, middle, center

```{r chp_2hrs_tweets_per_min_filtered_userpactime, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
# added 20170207
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% chp.event.2hrs),
     chp.event.2hrs, breaks.x = "15 mins",
     fill.under.line = FALSE, event.line.alpha = 0.6,
     subtitle.color = "#FF6666", subtitle.size = rel(0.8),
     plot.subtitle = "CHP stabbing occurred between 10:05am and 10:10am on 2 Feb",
     label.format.x = "%l:%M%P",
     plot.title = "Summary of tweets passing event AND location keyword filters\nand where users self-reported Pacific time zone",
     event.time = chp.event.times)
```

???


No signal activation for a High Profile, but low casualty event

Between 10:00 and 12:00 on this date, 343,084 tweets were collected (Fig 3). After filtering the text of tweets for those which contained at least one event-related keyword and at least one location-related keyword in the text of the tweet, the total number of tweets over the two hour period was reduced to 72,952. In the two hours following this event, a signal using our key generic words with restriction to location set at the 200 tweet per minute threshold would not have fired (Fig 3) indicating that use of this as a large scale multiple casualty signal for placing hospitals on standby would have worked correctly during a high profile, low casualty event. In addition, the signal would not have been activated even during the height of the Super Bowl activities (during game time) again supported it as a potential signaling threshold for early warning detection (Fig 4).

<!--
Plots below look in depth at tweets on Tue, Feb 2, 2016 between 10am and noon PST, when a CHP officer was stabbed in San Francisco between 10:05 and 10:10am (approximately). SFPD's first tweet about the stabbing appeared at 10:30am. 343,084 tweets were collected during this interval (10am to noon). After filtering for tweets which contain *both* a location and an event keyword in the text of the tweet (or original tweet, if the record is a retweet or quoted tweet), the number of records drops to 72,952. Filtering on only event keywords results in the retention of 192,251 records.
-->

On Tue, Feb 2, 2016, between 10:05 and 10:10am PST (approximately), a CHP officer was stabbed in San Francisco. SFPD's first tweet about the stabbing appeared at 10:30am.



---

# Conclusions

- Rapid use and scalability
- Geographic term included
- Proof of Concept Study
- Powerful, predictable, and potentially important resource for optimizing disaster response
- Leverage advanced analytics for prospective monitoring

???

Social media platforms including Twitter have great potential for use in formalized disaster planning and response. Social media data has demonstrated that this mechanism is a powerful, predictable, and potentially important resource for optimizing disaster response. Further investigation is warranted to assess the utility of prospective signal thresholds for hospital based activation.

Our findings confirm that information flows via Twitter extremely rapidly after an event of interest. In the 5 case events examined, the first relevant tweets appeared in under 2 minutes from the estimated start time of each event in the after action reports. These tweets were posted prior to 911 calls in all the events except for the Sandy Hook (SH) school shooting. Fig 1 demonstrates the rapid use and scalability and underscores the potential power of this modality for information dissemination. We believe that SH may have had a delayed scalability in the first hour following the event because this event took place at an elementary school where the number of persons on site during the time of the initial event with access to a device allowing posting to social media would have been extremely limited. In addition, the tweet signature is also likely a reflection of the number of secondary witnesses or the public nature of the disaster taking place. For those events occurring in highly populated areas or events, the number of potential witnesses and Twitter users will be greater than in events occurring in geographically isolated areas or low populated events.

This proof of concept study has shown that a non-traditional data source like Twitter could be utilized to develop early warning signals of multiple casualty events that require hospital based responses that exceed normal operations. A 200 tweet per minute threshold signal in this study would have resulted in hospital notification before patient arrival in all cases where injuries requiring hospital based care were present. In addition, it would have sent notification prior to or within 3 minutes of county disaster signals in cases where county wide alerts were activated. Simultaneous or nearly simultaneous notification is still valuable given that redundant communications are needed given the high rate of failure of infrastructure that often occurs following many mass casualty events.

Although more work is needed to identify the optimum tweet per minute threshold that would result in an appropriate sensitivity and specificity for a wide spectrum of hospital based disaster activations, a signal developed from generic trauma search terms coupled with terms that are geographically restricted has real promise. To be generalizable, our data utilized generic terms such as 'crash, accident, shot, shooting, stab, stabbed, stabbing, fall, dead, died, earthquake, flood, hurricane, tornado, victim, victims, fatality, fatalities, attack' coupled with a geographic restriction. However, to reflect the evolution of multiple casualty events since the original 5 included in this study occurred, we suggest expanding these generic terms to include 'terrorist' and 'terror.' It will be necessary if these signals are incorporated into practice, to continue to iterate on the most relevant terms as the safety and security of our world evolves.

While promising in concept, the data presented in this paper can not be used to determine the false negative rate of a signal applied across a spectrum of disaster scenarios. Disasters remain unpredictable in frequency making prospective testing of the signal challenging both from a data storage standpoint and practicality standpoint. These signals will be most helpful if they are geographically restricted, but predicting in advance that an event will definitely occur to allow testing of the adapted signal for a given region is nearly impossible. One could create a signal for a given location and no event ever takes place. In contrast, it is impractical without massive compute infrastructure to prospectively monitor all geographic areas in the world for the next large scale disaster. Thus, the determination of the false negative rate (failure to activate for a real event) will remain reliant on retrospective tweet data ascertained following future disaster events of varying types.

Although there are no standard approaches adopted widespread for the use of social media platforms for disaster planning and notification, there are many potential advantages to social media over traditional resources for communication of initial information in disaster times. Social media is inexpensive, rapidly disseminated, allows 2 way communication, scalable through 'retweets,' and not dependent upon a traditional power grid. In fact, there have been several worldwide incidences that have cited Twitter as the only source of communication available when other traditional communications have failed. In contrast, traditional media sources filter the message, are only one-way communication, have a slower dissemination, lack the same scalability, are dependent upon the power grid, and exclude geographically remote areas. Further, standard communication systems have failed in a number of recent events including the Taiwan typhoon in 2009, 2010 Haiti earthquake, and alternative communication sources including Twitter were relied upon heavily in the 2011 Egyptian uprising. Merchant et al. note that social media has great potential to increase 'our ability to prepare for, respond to, and recover from events that threaten the public's health.'

These social media tools coupled with advanced analytics have great promise for further development of prospective early warning disaster signals, but they also present a number of limitations. To be most effective, the signals need to be geographically specific and thus, would require some customization. In contrast, the search terms the signals are built from as demonstrated by our Super Bowl test of the signal can be useful even if they are quite generic. Events that occur in more remote geographic areas that lack access to internet would be less likely to be sensitive to a signal built on social media activity. Further, there is differential use of social media across differing age groups, socioeconomic statuses, and geographically. Finally, infrastructure disruption in wide spread disasters may limit the use of social media and responding agencies must have access to social media for this tool to be effective. Each of these could limit the applicability of these types of warnings.

---

# Further reading

<!--
<sup>1</sup>
-->

* `r TextCite(bib, "twittertrauma2017")`, "Finding the Signal in the Noise: Could Social Media Be Utilized for Early Hospital Notification of Multiple Casualty Events?"
* `r TextCite(bib, "asianacrash2016")`, "Reconsidering the resources needed for multiple casualty events: Lessons learned from the crash of Asiana airlines flight 214"

<!--
.footnote[1: `r TextCite(bib, "twittertrauma2017")`]
-->

---
layout: false
class: inverse, fullscreen, middle, center

# Questions?

---

# References

.small[

```{r printbib, results="asis", echo=FALSE, cache=FALSE}
PrintBibliography(bib, .opts = list(check.entries = FALSE, sorting = "ynt", no.print.fields = c("doi")))
```

]

---

# Credits

#### Built using [xaringan](https://github.com/yihui/xaringan) with
+ [knitr](http://yihui.name/knitr),
+ [R Markdown](https://rmarkdown.rstudio.com),
+ the [remark.js](https://github.com/gnab/remark/) framework,
+ the [RefManageR](https://github.com/ropensci/RefManageR/) bibliography manager, and
+ the [highlight.js](https://highlightjs.org/) syntax highlighter.

---

# Session Info

```{r sessionInfo, results="markup", echo=FALSE, message=FALSE, warning=FALSE}
library(pander)
pander(sessionInfo(), locale = FALSE, compact = TRUE)
```