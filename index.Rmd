---
title       : Twitter-based Alerts of Major Disasters
subtitle    : Biomedical Big Data Seminar
author      : Rachael A. Callcut, MD, MSPH &amp; Sara E. Moore, MA
date        : 13 November 2017
output:
  xaringan::moon_reader:
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    css: ["default", "custom.css"]
    nature:
      beforeInit: "macros.js"
      highlightStyle: tomorrow-night-eighties # arta, ascetic, dark, default, far, github, googlecode, idea, ir_black, magula, monokai, rainbow, solarized-dark, solarized-light, sunburst, tomorrow, tomorrow-night-blue, tomorrow-night-bright, tomorrow-night, tomorrow-night-eighties, vs, zenburn
      highlightLines: true
      countIncrementalSlides: false
---

```{r knitr_setup, echo=FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.height = 6,
					  dpi = 300, fig.cap="", fig.align='center')
showtext::showtext_opts(dpi = 300)
```

```{r loadbib, echo=FALSE, cache=FALSE}
library(RefManageR)
bib = ReadBib("references.bib", check = FALSE)
BibOptions(check.entries = FALSE, style = "markdown",
		   cite.style = "authoryear", bib.style = "numeric")
```


# How to access these slides

### Via git:

```{bash, eval=FALSE}
git clone https://github.com/saraemoore/TwitterTraumaBBD2017.git
```

### Download directly:
https://github.com/saraemoore/TwitterTraumaBBD2017/archive/master.zip

### View online:
http://www.saraemoore.com/TwitterTraumaBBD2017

???

* I won't be showing all the code on the slides today, but it's all available in the R Markdown document used to create these slides in this repository on github, or in the corresponding R script that's also in the github repository.

---
class: inverse, fullscreen, middle, center

> "A mass casualty incident is defined as an event which generates more patients at one time than locally available resources can manage using routine procedures. It requires exceptional emergency arrangements and additional or extraordinary assistance." &mdash; .small[`r TextCite(bib, "whomci2007")`]

---

layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

<!--
  https://git.io/vFEx0
-->

---

# July 6, 2013

.left-column[
- Asiana Flight 214
- Boeing 777-200 ER
- Incheon, South Korea
- 307 persons aboard
    - 291 passengers
    - 16 crew
    - 70 Children

]

.right-column[

![:scale 60%](assets/img/image3.jpg)

![:scale 90%](assets/img/image4.jpg)
]

---

# "11:28 am Plane crash"

- 1st official airport notification: "Code 33: Plane down"
- 1st hospital notice: police officer in ED
- Confusing initial information:
    - 'small cargo plane'
    - 'large cargo plane'
    - 'commercial jet liner'
    - 'Everyone is okay'

.pull-left-wide[
![](assets/img/image5.png)
]

.pull-right-wide[
![](assets/img/sfocrash_after.jpg)
]

???

In the SF Asiana airplane crash, the vast majority of the early information from the event was erroneous. 'Disaster myths' are harmful and they can alter the way a community responds to an event. Although people do not intend to mislead, disasters are emotional, making perception of facts often skewed.

---

```{r fixcitation1, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
TextCite(bib, "houstonetal2015", .opts = list(max.names = 1))
```

# What do we do?

- Watch the news?
- Check the internet?
- Wait for the 'official notification' to prepare?
- Don't worry about it?

> "Effective disaster communication may prevent a disaster or lessen its impact, whereas ineffective disaster communication may cause a disaster or make its effects worse." &mdash; .small[`r TextCite(bib, "houstonetal2015", .opts = list(max.names = 1))`]

???

Communications in times of mass casualty events, especially those involving natural disasters, have always posed challenges. In nearly every after action report, communication is cited as the most common entity needing improvements

---

# Instantaneous News

.pull-left-wide[
![:scale 98%](assets/img/instant_news2.jpg)
]

.pull-right-wide[
![:scale 98%](assets/img/instant_news1.jpg)
]



<!--

- 1st Twitter message/photo **30 seconds** after impact
- Retweeted 4,450 times in first 24 hours

    https://www.usatoday.com/story/news/nation/2013/07/07/social-media-asiana-airlines-tweets-david-eun-twitter-tweets/2496903/
-->

???

This information nearly always precedes the activation of any formal notification process. As an example, the county-wide disaster page activation occurred nearly 30 minutes later than information appeared on social media feeds following the crash of a commercial jet liner in San Francisco with over 300 potentially injured occupants. In fact, the first tweet of a plane down occurred 30 seconds after the crash and preceded even the 911 calls. Drawing on the experience from the Asiana plane crash, we recognized the potential power of this modality for earlier hospital notification.

---

# Modern Society's Digital Footprint

.pull-left[
- Every day, on average:
    - 269 billion emails are sent<sup>1</sup>
    - 1.37 billion people log on to Facebook<sup>2</sup>
    - over 500 million Tweets are posted<sup>3</sup>
    - over 500 million people log on to Instagram<sup>4</sup>
]

.pull-right[
![](assets/img/online_in_60s_2.jpg)
]

.footnote[1: [The Radicati Group, Feb 2017](http://www.radicati.com/wp/wp-content/uploads/2017/01/Email-Statistics-Report-2017-2021-Executive-Summary.pdf)<br />2: [Facebook, June 2017](https://newsroom.fb.com/company-info/)<br />3: [Twitter, 2016](https://business.twitter.com/en/basics.html)<br />4: [Instagram, 2017](https://instagram-press.com/our-story/)]

<!--
    https://www.allaccess.com/merge/archive/26034/what-your-audience-is-doing-when-they-re-not
-->

???

Social media plays an ever increasing role in our society with 69% of Americans using social media including approximately 78 million persons using it multiple times per day. (http://www.pewinternet.org/fact-sheet/social-media/)

Each minute it is estimated that hundreds of thousands tweets are sent in the U.S. Social media is a highly interactive platform which is web based and allows two way communication between users.

---
layout: false
class: inverse, fullscreen, middle, center

# The ability to detect the signal in the noise is the key...

???

The difficulty for hospital based providers in the age of widespread availability of social media is trying to wade through the enormous amount of information that flows almost instantaneously following these events.

---
layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

---

# Using Twitter to Track Events

```{r fixcitation2, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
TextCite(bib, "twitterdengue2017", .opts = list(max.names = 1))
```

.pull-left[
- 'Microblogging'
    - Two-way communication
- Non-disaster public health events
]

.pull-right[
![](assets/img/journal.pntd.0005729.g001b.png)
]

![:scale 75%](assets/img/journal.pntd.0005729.g004.png)

.footnote[`r TextCite(bib, "twitterdengue2017", .opts = list(max.names = 1))`]

???

Twitter has also been described as a form of 'microblogging' that allows 'dissemination of information that is timely and vast' and it has become a highly utilized format by the public following worldwide mass casualty events.

Twitter allows two-way communication has been used to track non-disaster public health events, such as in this study published earlier this year, where tweet volume was used to forecast cases of Dengue fever.

---

# Using Twitter to Track Disasters

.pull-left[
- Information delivery
    - 2009: Typhoon Morakot in Taiwan
    - 2010: Earthquake in Haiti
    - 2011: Egyptian uprising

- Federal agencies
    - FEMA
    - National Weather Service
    - US Government (2010)
]

.pull-right[
[![:scale 60%](assets/img/704px-Typhoon_Morakot_Aug_7_2009.jpg)](http://rapidfire.sci.gsfc.nasa.gov/gallery/?2009219-0807/Morakot.A2009219.0525.2km.jpg)
![](assets/img/embassy_sfocrash.png)
]

<!--
    `r TextCite(bib, "taiwantyphoon")`
    https://www.pbs.org/newshour/world/haiti-quake-propels-twitter-community-mapping-efforts
    https://www.wired.com/2010/01/texts-tweets-saving-haitians-from-the-rubble/
-->

???

Several prior case studies have been published detailing experiences with the platform ranging from directing rescuers to victims following typhoons, hurricanes, earthquakes, and terrorist events, to using the platform to exchange information about how to reach disaster response resources for recovery. To date, research that has been conducted on using these powerful tools during times of disaster are limited to epidemiologic reports of basic Twitter use statistics and characteristics of users.

Social media has played an integral role in several prior large scale disasters including the the 2011 Egyptian uprising, Taiwan typhoon in 2009, and in the 2010 Haiti earthquake. In each of these cases, traditional communication methods were inadequate and the use of social media improved coordination efforts of disaster response and recovery. Similarly, social media has been successfully utilized in other fields to predict and respond to public health events.

---
layout: false
class: inverse, fullscreen, middle, center

# Can social media be utilized for early hospital notification of multiple casualty incidents (MCIs)?

???

Advocates believe that social media tools have great promise for enhancing our current disaster management communication strategies but studies are lacking. These concepts have remained largely theoretical and the limited prior data available have focused on the descriptions of the public use of social media following various worldwide crises. Frequently, delayed notification and lack of early information has hindered timely hospital based activations in large scale multiple casualty events. Although many institutions and federal agencies have incorporated a social media response to deliver information to the public following an event, there has been no focused research on leveraging this platform to develop improved and early disaster notification mechanisms to law enforcement, prehospital providers, or hospital systems.

---
layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

---

# Objectives

**Eventual goal**: use real-time social media data to

* detect MCIs and/or
* predict location-specific trauma center volume.

**Immediate goal**:

* determine if there is sufficient 'signal' in Twitter feeds to develop a prospective multiple casualty warning system for hospital-based activation.

???

We hypothesized that Twitter real-time data would produce a unique and reproducible signal within minutes of multiple casualty events and we investigated the timing of the signal compared with other hospital disaster notification mechanisms.

The objective of this study was to 1) initiate the development of a qualitative model designed to provide an additional tool for hospital level disaster notification and 2) to assure that the model would not falsely activate in a control event.

---

# Methods

- Retrospective
    - Five recent (2012-2014) U.S. Multiple Casualty Incidents
    - Event initiation $\rightarrow$ 7 days post-event

- Prospective
    - Super Bowl 50 (San Francisco, Feb 2016)
    - Approximately 7 days prior to and 7 days after Super Bowl kickoff

???

Historical analysis to develop an early warning signal followed by data collection to prospectively 'test' the signal thresholds

---
layout: false
class: inverse, fullscreen, middle, center

# Getting the data

---
layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

---

# Acquiring Twitter data: Free

* Historical APIs
    - Conduct singular searches, read user profile information, or post Tweets. Typically RESTful.
    - [7-Day Search API](https://developer.twitter.com/en/docs/tweets/search/overview/basic-search): [GET search/tweets](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets) (returns Tweets matching a specified query)
    - Limitations: [Rate limited](https://developer.twitter.com/en/docs/basics/rate-limits.html) (max number of queries every 15 mins); max 100 tweets returned per query; not exhaustive
* [Streaming APIs](https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data)
    - Monitor or process Tweets in real-time.
    - [Filter](https://developer.twitter.com/en/docs/tweets/filter-realtime/overview): [POST statuses/filter](https://developer.twitter.com/en/docs/tweets/filter-realtime/api-reference/post-statuses-filter.html) (returns public statuses matching $\geq 1$ filter predicates)
    - [Sample](https://developer.twitter.com/en/docs/tweets/sample-realtime/api-reference): [GET statuses/sample](https://developer.twitter.com/en/docs/tweets/sample-realtime/overview/GET_statuse_sample) (small random sample of public statuses)
    - Limitations: one connection at a time allowed per account, subject to streaming cap (max out at small percentage of total tweet volume), frequent reconnects may result in rate limiting/brief IP blocking

???

I've listed the MOST COMMON API endpoints. Others exist, typically for more specialized applications.

"Representational state transfer (REST) or RESTful web services are a way of providing interoperability between computer systems on the Internet. REST-compliant Web services allow requesting systems to access and manipulate textual representations of Web resources using a uniform and predefined set of stateless operations."

---

# Acquiring Twitter data: Enterprise

* Gnip: acquired by Twitter in 2014
* [Historical APIs](https://developer.twitter.com/en/docs/tutorials/choosing-historical-api)
    - Conduct singular searches, read user profile information, or post Tweets. Typically RESTful.
    - Search API: [30-Day](https://developer.twitter.com/en/docs/tweets/search/overview/30-day-search) or [Full-Archive](https://developer.twitter.com/en/docs/tweets/search/overview/full-archive-search)
    - Batch/job-based: [Historical PowerTrack](https://developer.twitter.com/en/docs/tweets/batch-historical/overview)
* [Streaming APIs](https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data)
    - Monitor or process Tweets in real-time.
    - [Filter](https://developer.twitter.com/en/docs/tweets/filter-realtime/overview): [Realtime PowerTrack](https://developer.twitter.com/en/docs/tweets/filter-realtime/api-reference/powertrack-stream) (aka filtered 'Firehose')
    - [Sample](https://developer.twitter.com/en/docs/tweets/sample-realtime/api-reference): [Decahose stream](https://developer.twitter.com/en/docs/tweets/sample-realtime/overview/decahose) (10% random sample of 'Firehose')

???

"Historical PowerTrack (HPT) is built to deliver Tweets at scale using a batch, Job-based design where the API is used to move a Job through multiple phases. A data file is generated for each 10-minute period that contains at least one Tweet. "

"Full-Archive Search (FAS) delivers matched Tweets in a manner analogous to Google Search. Full-Archive Search... responds with a subset of Tweets, with the ability to paginate for more Tweets as needed."

As of April, 2015, 'firehose' access is cut off for all third-party resellers. Now it all goes through Gnip, which was acquired by Twitter in May, 2014. ([1](http://www.forbes.com/sites/benkepes/2015/04/11/how-to-kill-your-ecosystem-twitter-pulls-an-evil-move-with-its-firehose/), [2](http://thenextweb.com/dd/2015/04/11/twitter-cuts-off-firehose-resellers-as-it-brings-data-access-fully-in-house/), [3](http://www.infoworld.com/article/2908869/big-data/twitters-firehose-shut-off-is-the-newest-hazard-of-the-api-economy.html))

---

# Processing Twitter data (in R)

.pull-left[

* Parsing JSON (javascript object notation) files:
    * **jsonlite** ([CRAN](https://CRAN.R-project.org/package=jsonlite)/[github](https://github.com/jeroen/jsonlite))
    * rjson ([CRAN](https://CRAN.R-project.org/package=rjson))
    * RJSONIO ([CRAN](https://CRAN.R-project.org/package=RJSONIO))
* REST APIs:
    * **twitteR** ([CRAN](https://CRAN.R-project.org/package=twitteR)/[github](https://github.com/geoffjentry/twitteR)): now deprecated
    * RTwitterAPI ([github](https://github.com/joyofdata/RTwitterAPI))
* Streaming APIs:
    * **streamR** ([CRAN](https://cran.r-project.org/web/packages/streamR/index.html)/[github](https://github.com/pablobarbera/streamR))
    * tweet2r ([CRAN](https://cran.r-project.org/web/packages/tweet2r/index.html)/[github](https://github.com/pauarago/tweet2r))
* Either:
    * rtweet ([CRAN](https://CRAN.R-project.org/package=rtweet)/[github](https://github.com/mkearney/rtweet))
]


.pull-right[

* Authenticating via OAuth:
    * ROAuth ([CRAN](https://cran.r-project.org/web/packages/ROAuth/index.html)/[github](https://github.com/geoffjentry/ROAuth))
    * httr ([CRAN](https://CRAN.R-project.org/package=httr)/[github](https://github.com/r-lib/httr/))
* Other Resources:
    * [Analyzing and Collecting Social Media Data with R](https://github.com/pablobarbera/social-media-workshop)
    * [Accessing Data from Twitter API using R](https://medium.com/@GalarnykMichael/accessing-data-from-twitter-api-using-r-part1-b387a1c7d3e)
    * CRAN's ["Web Technologies" package collection](https://cran.r-project.org/web/views/WebTechnologies.html), particularly the "parsing Data from the Web" and "social media" sections
]

- Not in R?: See Twitter's list of tools for [other programming languages](https://dev.twitter.com/overview/api/twitter-libraries)

---
layout: false
class: inverse, fullscreen, middle, center

# Exploratory Analysis of Historical Twitter data

---
layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

---

# MCIs of interest

- Sandy Hook Elementary School shooting (**SH**)
    - 14 Dec 2012 @ 09:35:39 EST ([time of first 911 calls](http://www.ct.gov/csao/lib/csao/compressed-sandy-hook-report.pdf))
    - ["shortly after 10am"](https://www.huffingtonpost.com/2012/12/14/danbury-hospital-school-shooting_n_2304756.html): Hospital notification of school shooting
- Boston Marathon bombing (**BB**)
    - 15 Apr 2013 @ 14:49 EDT ([detonation time of first bomb](http://www.mass.gov/eopss/docs/mema/after-action-report-for-the-response-to-the-2013-boston-marathon-bombings.pdf))
    - 14:53 EDT: Hospital MCI notification
- Asiana Airlines 214 crash at SFO (**SF**)
    - 6 Jul 2013 @ 11:28 PDT ([seawall impact time](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR1401.pdf))
    - 12:03 PDT: Hospital MCI notification
- South Napa earthquake (**NE**)
    - 24 Aug 2014 @ 03:20:44 PDT ([earthquake origin time](http://earthquake.usgs.gov/earthquakes/eventpage/nc72282711))
    - 04:08 PDT: Emergency Operation Center activation
- Marysville Pilchuck High School shooting (**MV**)
    - 24 Oct 2014 @ 10:39 PDT ([time of first 911 call](https://everettwa.gov/DocumentCenter/View/3915))
    - 11:07 PDT: Schools send alert to community that high school is in lockdown

???

The five multiple casualty events of interest included the Boston Bombing, San Francisco Asiana 214 Plane Crash, Napa Earthquake, Sandy Hook Elementary School shooting, and Marysville School Shooting. Relevant tweets posted on the day of the event and in the six days thereafter (for seven days total per event) were identified using the incident time of each event (as displayed here).

https://www.calhospital.org/sites/main/files/file-attachments/napa_earthquake_presentation_2up.pdf
http://www.heraldnet.com/news/marysville-pilchuck-high-school-shootings-timeline/

---

# Developing the Signal

- Feb 2015: data acquired post hoc from Twitter via Gnip
    - Historical PowerTrack (*purchased*)
- 100% capture based on keywords
- Complete tweet content, including:
    - tweet text
    - timing
    - user information (such as # of followers)
    - geocoding (when available/provided)
    - retweet/quote tweet info

???

Tweets relating to these recent U.S. multiple casualty events were acquired post hoc from Twitter via Gnip's Historical PowerTrack. All data was purchased from Twitter. The tweet content including the text of the tweet, user information, geocoding of the tweet, structure of the tweet, number of each user's followers, retweet counts, and timing of the tweets were supplied.

---

# Keywords for MCIs

.smaller[

Disaster Event                                    | Search Terms
------------------------------------------------- | ---------------------------------------------
**Sandy Hook Elementary School shooting (SH)**    | School (gunman OR 'Sandy Hook' OR victims OR shooting OR Newtown)
                                                  | Newtown (shooting OR gunman OR student OR Sandy Hook OR elementary)
**Boston Marathon bombing (BB)**                  | Bomb (Boston OR marathon OR explosion OR terrorist OR finish line)
                                                  | Boston (explosion OR terrorist)
**Asiana Airlines crash (SF)**                    | Plane (crash OR SFO OR runway OR San Francisco OR Asiana OR '214')
                                                  | Asiana (Crash OR '214' OR Runway OR SFO OR San Francisco)
**South Napa earthquake (NE)**                    | Earthquake (San Francisco OR 'sf' OR Napa OR '6.0')
**Marysville Pilchuck High School shooting (MV)** | School (gunman OR Marysville OR victims OR shooting OR Washington)
                                                  | Marysville (shooting OR gunman OR student)
]

???

Tweets were identified using disaster-specific filtering rules. Where more than one text pattern was listed for a given event, a tweet was included if it matched on either (or both). Filtering was performed via a case-insensitive keyword search where individual keywords were separated by boolean operators. A space denotes an 'and' operator, an uppercase 'OR' denotes an 'or' operator, and this logic is combined using parentheses to group clauses. Quotation marks indicate terms which should be matched exactly (not tokenized).

<!--


# Analysis

- Mostly qualitative, with objective of identifying similarities across events
- Differing tweet per minute thresholds were compared to:
    - County disaster notification time (if a county disaster page was activated)
    - Hospital stand by notification times (if no county disaster page was performed)
    - Patient arrival times (obtained from after action reports).

-->

---

# Results

```{r twittereventmetadata, message=FALSE, echo=FALSE, warning=FALSE}
# http://www.mass.gov/eopss/docs/mema/after-action-report-for-the-response-to-the-2013-boston-marathon-bombings.pdf
# "Monday, April 15, 2013: 2:49pm First bomb detonates. Second bomb detonates 13 seconds later."
# https://en.wikipedia.org/wiki/Marysville_Pilchuck_High_School_shooting
# 
# http://earthquake.usgs.gov/earthquakes/eventpage/nc72282711#general_summary
# "2014-08-24 10:20:44 (UTC)"
# http://cspsandyhookreport.ct.gov/
# https://en.wikipedia.org/wiki/Sandy_Hook_Elementary_School_shooting
# 
# http://www.ntsb.gov/news/events/Pages/2014_Asiana_BMG-Abstract.aspx
# "On July 6, 2013, about 1128 Pacific daylight time"

# see also
# https://en.wikipedia.org/wiki/List_of_UTC_time_offsets

twitterEventInfo <- data.frame(shortName = c("boston", "marysville", "napa", "sandyhook", "sfo"),
  longName = c("Boston Marathon bombing", "Marysville shooting", "Napa earthquake", "Sandy Hook shooting", "SFO crash"),
  localTime = as.POSIXlt(c("2013-04-15 14:49:00-0400", # EDT
               "2014-10-24 10:39:00-0700", # PDT
               "2014-08-24 03:20:44-0700", # PDT
               "2012-12-14 9:35:00-0500", # EST
               "2013-07-06 11:28:00-0700"), #PDT
               format = "%Y-%m-%d %H:%M:%S%z",
               tz = "UTC"),
  shortTZ = c("EDT", "PDT", "PDT", "EST", "PDT"),
  scalesTZ = paste("America", c("New_York", "Los_Angeles", "Los_Angeles", "New_York", "Los_Angeles"), sep = "/"),
  twitterTZ = paste(c("Eastern", "Pacific", "Pacific", "Eastern", "Pacific"), "Time (US & Canada)"),
  stringsAsFactors = FALSE)
```

```{r loaddata_twitterevents, message=FALSE, echo=FALSE, warning=FALSE}
baseDir = file.path(getwd(), "..")

# using preprocessed data files to speed up rmd rendering
tweetsPerMinAll = readRDS(file.path(baseDir, "twitter_data", "event_tweets_per_min.rds"))
tweetMapDataAll = readRDS(file.path(baseDir, "twitter_data", "tweet_map_data.rds"))

# tweet.df = sapply(twitterEventInfo$shortName,
#     function(x) readRDS(file.path(baseDir, "twitter_data", paste(paste(x, "tweet_data", sep = "_"), "rds", sep = "."))),
#     simplify = FALSE)
twitter_entities.hashtags.combo = sapply(twitterEventInfo$shortName,
    function(x) readRDS(file.path(baseDir, "twitter_data", paste(paste(x, "hashtag_data", sep = "_"), "rds", sep = "."))),
    simplify = FALSE)
```

- 3.8 million tweets across 5 events
    - SH: $n = 1,815,751$
    - BB: $n = 1,147,793$
    - SF: $n = 430,616$
    - MV: $n = 249,847$
    - NE: $n = 205,073$
- 47.1% (**SF**) to 58.4% (**BB**) retweets
- Mean user followers: 3,382 (**BB**) to 9,992 (**MV**)
    - Median: 226 (**SH**) to 417 (**MV**)
- Mean user tweets: 13,253 (**SH**) to 46,445 (**MV**)
    - Median: 4,043 (**SH**) to 12,934 (**MV**)
- 0.8% to 1.1% of tweets geocoded for 4 of 5 events
    - **NE**: 1.8% geocoded
- Most popular utility/app used to post tweets: **Twitter for iPhone**

???


Tweet data was packaged as a collection of compressed JSON files, each containing tweets from one 10 minute span of a single event's seven day collection period. Data within these JSON files was imported and cleaned using R with package jsonlite. Any tweets posted prior to their respective event's time were excluded from the analytic sample. One observation corresponds to one tweet (or retweet).

Over 3.8 million tweets were analyzed in total. Tweets occurred very rapidly for all events (<2 mins) and represented 1% of the total event specific tweets in a median of 13 minutes of the first 911 calls. The distribution of tweets included unique tweets in 42-53% of postings and the remaining 47-58% were retweets or reposting of prior tweets. The median followers of the person originating a tweet ranged from 226 to 417 across events.

---
layout: false

class: fullscreen, middle, center

<!--
[Link](file:///Users/fpgcdi/Dropbox/Trauma\ and\ Coagulation\ \(White\ Space\ Conflict\)/Twitter/prelim_analysis/index.html)
-->

```{r twittertsplot_libsandfxns, message=FALSE, echo=FALSE, warning=FALSE}
library(lubridate)
library(ggplot2)
library(scales)
# library(grid)
# library(zoo)
# library(data.table)
# library(dplyr)
library(dygraphs)
library(xts)

theme_twitter = theme_minimal(base_size = 12) +
    theme(legend.position = "bottom",
      # plot.title = element_text(hjust = 0.5), # ggplot 2.2.0
      # plot.subtitle = element_text(hjust = 0.5), # ggplot 2.2.0
      # axis.text.x = element_text(angle = 90),
      axis.line = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.grid.major.y = element_line(colour = "white", size = 0.3),
          panel.grid.minor = element_blank(),
          strip.background = element_blank(),
      axis.text = element_text(margin = unit(-0.25, 'cm')),
          panel.ontop = TRUE)

theme_twitter_nolines = theme_twitter %+replace% theme(panel.grid.major.y = element_blank())

makeHLineAnnotation = function(x.interval, y, color, label = "max", alpha = 0.8,
                 linetype = 2, size = 3, hjust = -0.02, vjust = -0.7) {
  x = int_start(x.interval)
  list(
      geom_hline(yintercept = y,
           color = color,
           alpha = alpha,
           linetype = linetype),
      annotate("text", x = x,
             y = y,
             label = label,
             color = color,
                   alpha = alpha,
                   size = size,
                   hjust = hjust,
                   vjust = vjust)
  )
}

makeVLineAnnotation = function(x, y, label, color = "red",
                             alpha.label = 0.8, alpha.line = 0.4, size = 3.5,
                             angle = 90, hjust = 0.5, vjust = -0.15) {
  vannot = vector("list", 2)
  if(length(x) > 1) {
    vannot[[1]] = geom_polygon(data = data.frame(
        created_at = rep(x, each = 2),
        n = c(-Inf, Inf, Inf, -Inf)),
      fill = color, alpha = alpha.line)
  } else {
    vannot[[1]] = geom_vline(
      xintercept = as.numeric(x),
      color = color, alpha = alpha.line)
  }

  if(!is.null(label)) {
    vannot[[2]] = annotate("text", x = max(x),
        y = y, label = label,
        colour = color, alpha = alpha.label, size = size,
        vjust = vjust, hjust = hjust, angle = angle)
  } else {
    vannot = vannot[1]
  }
  return(vannot)
}

makeTwitterTSPlot = function(df, plot.time.interval, fill.under.line = TRUE,
               multi.line = FALSE, include.max.annot = TRUE,
                           event.label = NULL, event.time = NULL, adjust.y.max = 1.05,
                           event.label.y = mean(range(df$n)),
                           event.label.size = 3.5,
                           event.label.alpha = 0.8, event.line.alpha = 0.4,
                           event.label.color = "red", event.label.angle = 90,
                           event.label.vjust = -0.15, event.label.hjust = 0.5,
                           line.colors = c("gray20", "#5aae61"), axis.text.x.angle = 90,
                           plot.title = "Summary of tweets passing event AND location keyword filters",
                           plot.subtitle = "and where users self-reported Pacific time zone",
               subtitle.color = NULL, subtitle.align = 0.5, subtitle.size = rel(0.9),
               title.align = 0.5, breaks.x = "1 hours", label.format.x = "%e %b %l%P",
                           tz.to.use = "America/Los_Angeles") {

  p = ggplot(df, aes(x = with_tz(created_at, tz.to.use), y = n)) +
    scale_x_datetime("Posted time (local time zone)",
             timezone = tz.to.use,
             date_breaks = breaks.x,
             expand = c(0.01, 0),
             date_labels = label.format.x) +
    ggtitle(plot.title, subtitle = plot.subtitle)

  max.y.actual = max(df$n)
  if(fill.under.line) {
    p = p + theme_twitter +
            theme(plot.title = element_text(hjust = title.align),
                plot.subtitle = element_text(hjust = subtitle.align, size = subtitle.size),
                axis.text.x = element_text(angle = axis.text.x.angle)) +
            geom_bar(fill = line.colors[1],
                   width = 60, stat="identity")
  } else {
    p = p + theme_twitter_nolines +
            theme(plot.title = element_text(hjust = title.align),
                plot.subtitle = element_text(hjust = subtitle.align, size = subtitle.size),
                axis.text.x = element_text(angle = axis.text.x.angle))
    if(!is.null(subtitle.color)) {
      p = p + theme(plot.subtitle = element_text(colour = subtitle.color))
    }
    if(multi.line){
      p = p + geom_line(aes(color = count.type), size = 0.75) +
           scale_colour_manual("Tweet Count:", values = line.colors)
        if(include.max.annot) {
          other.levels = setdiff(levels(df$count.type), "Actual")
          for(i in seq_along(other.levels)) {
            p = p +
              makeHLineAnnotation(x.interval = plot.time.interval,
                              y = max(subset(df, count.type==other.levels[i])$n),
                              color = line.colors[i+1])
          }
        }
      max.y.actual = max(subset(df, count.type=="Actual")$n)
    } else {
      p = p + geom_line(size = 0.5, color = line.colors[1])
    }
  }

  if(include.max.annot) {
    p = p +
    makeHLineAnnotation(x.interval = plot.time.interval,
                    y = max.y.actual,
                    color = line.colors[1])
  }
  p = p +
    scale_y_continuous("Number of tweets per minute",
               limits = c(0, max.y.actual*adjust.y.max))

  if(!is.null(event.time)) {
    p = p + makeVLineAnnotation(x = event.time, y = event.label.y, label = event.label,
                            color = event.label.color,
                            alpha.label = event.label.alpha, alpha.line = event.line.alpha,
                  size = event.label.size, angle = event.label.angle,
                  hjust = event.label.hjust, vjust = event.label.vjust)
  }

  return(p)
}

# _tweets_per_min_line
makeTweetsPerMinLinePlot <- function(eventTweetsPerMin, eventMetadata) {
  eventPlus12hrs = interval(eventMetadata$localTime, eventMetadata$localTime + hours(12))

  makeTwitterTSPlot(subset(eventTweetsPerMin, created_at < int_end(eventPlus12hrs)),
      eventPlus12hrs,
      fill.under.line = FALSE,
      subtitle.color = "#FF6666", subtitle.align = 0, subtitle.size = rel(0.8),
      event.line.alpha = 0.6, event.time = int_start(eventPlus12hrs), adjust.y.max = 1.05,
      plot.title = "Summary of tweets passing filter(s) posted in first 12 hours after event",
      plot.subtitle = paste(eventMetadata$longName, "occurred at approx.",
                            format(lubridate::with_tz(int_start(eventPlus12hrs), tz = eventMetadata$scalesTZ),
                          "%l:%M%P on %e %b %Y", tz = eventMetadata$scalesTZ)),
      tz.to.use = eventMetadata$scalesTZ)
}
```


[
```{r tweetspermin_lineplot5, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
i = 5

makeTweetsPerMinLinePlot(tweetsPerMinAll[[twitterEventInfo[i, "shortName"]]],
                         twitterEventInfo[i,])
```
](tweetspermin_dygraph5.html)

```{r sfo_dygraph_tweetspermin, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

eventName = "sfo"
eventMetadata = subset(twitterEventInfo, shortName==eventName)
tweets.per.min.xts = xts::xts(tweetsPerMinAll[[eventName]][,-which(colnames(tweetsPerMinAll[[eventName]])=="created_at"), drop = FALSE],
    order.by = tweetsPerMinAll[[eventName]][,"created_at"],
    tzone = eventMetadata$scalesTZ)

# library(highcharter)
# options(
#     highcharter.global = list(
#       # getTimezoneOffset = NULL,
#       # timezoneOffset = 0,
#       useUTC = FALSE
#       )
#     )
# highchart(type = "stock") %>% hc_add_series(tweets.per.min.xts, type = "line", name = "SFO crash") %>% hc_xAxis(type = "datetime") %>% hc_plotOptions(series = list(marker = list(enabled = FALSE)), line = list(dataGrouping = list(enabled = FALSE)))

p = dygraph(tweets.per.min.xts,
    main = paste(eventMetadata$longName, "Tweets passing filter(s) after event", sep = ": "),
    ylab = "Number of tweets per minute",
    width = 900, height = 600) %>% # default: width = 2400px, height = 1500px
    dyAxis("x", drawGrid = FALSE,
        label = paste0("Posted time (", eventMetadata$shortTZ, " time zone)"),
        rangePad = 1) %>%
    dyRangeSelector(dateWindow = c(eventMetadata$localTime - minutes(10), eventMetadata$localTime + hours(12))) %>%
    dyHighlight(highlightSeriesOpts = list(strokeWidth = 1.5),
        highlightCircleSize = 3,
        highlightSeriesBackgroundAlpha = 0.8) %>%
    dyEvent(eventMetadata$localTime, "Event", labelLoc = "top") %>% # labelLoc = "bottom"
    dyEvent(parse_date_time("July 6 2013 12:03", "b d Y H:M", tz = "America/Los_Angeles"),
        "Disaster page", labelLoc = "top") %>% # labelLoc = "bottom"
    dyOptions(includeZero = TRUE, useDataTimezone = TRUE,
        fillGraph = TRUE, fillAlpha = 0.3, colors = "#3B7EA1") %>% # founder's rock
    dyLegend(show = "always", hideOnMouseOut = FALSE)

htmlwidgets::saveWidget(p, "tweetspermin_dygraph5.html")

```


```{r tweetspermin_150mins_disasterpage_sfo, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}

eventName = "sfo"
eventMetadata = subset(twitterEventInfo, shortName==eventName)
sfoPlus150mins = interval(eventMetadata$localTime, eventMetadata$localTime + minutes(150))
p.sfo = makeTwitterTSPlot(subset(tweetsPerMinAll[[eventName]], created_at < int_end(sfoPlus150mins)),
    sfoPlus150mins,
    fill.under.line = FALSE, include.max.annot = FALSE,
        event.line.alpha = 0.6, event.label.vjust = 1.5,
        event.time = int_start(sfoPlus150mins),
        adjust.y.max = 1.05, event.label.y = max(tweetsPerMinAll[[eventName]]$n)*0.8,
        breaks.x = "30 mins", label.format.x = "%l:%M%P",
    plot.title = NULL, plot.subtitle = NULL,
    event.label = "SF Plane Crash",
        tz.to.use = eventMetadata$scalesTZ)

# Disaster page was at 12:03pm
p.sfo + makeVLineAnnotation(eventMetadata$localTime + minutes(33),
  max(tweetsPerMinAll[[eventName]]$n)*0.8,
  # mean(range(subset(tweetsPerMinAll[[eventName]], created_at < int_end(sfoPlus150mins))$n)),
  label = "Disaster Page", color = "darkgreen", vjust = 1.5, alpha.line = 0.6, alpha.label = 0.8)
```

???

To better structure visualizations for qualitative identification of common event 'signatures,' tweets were summarized into tweets per minute during the first twelve hours after the event onset. The speed with which tweet volume reached a threshold and/or peak was compared to the timing of the traditional county disaster signal for each event.

For multiple casualty events that require the resources beyond normal functioning, county governments in conjunction with local official coordinate disaster wide responses involving multiple jurisdictions and hospital systems. Each of the events utilized in this report generated a detailed after action report that produced by the responsible governing body. These reports can include detailed information on timing of the initial event, first EMS calls, hospital standby notification, and county wide disaster activation (if activated). Not all 'after action' reports report all metrics. These after action reports are considered the official documentation of the time line of the events and therefore, we utilized the metrics available in each report. 

---
class: fullscreen, middle, center

[
```{r tweetspermin_lineplot3, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
i = 3

makeTweetsPerMinLinePlot(tweetsPerMinAll[[twitterEventInfo[i, "shortName"]]],
                         twitterEventInfo[i,])
```
](tweetspermin_dygraph3.html)

```{r dygraph_tweetspermin3, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

i = 3
eventMetadata = twitterEventInfo[i,]
eventName = eventMetadata$shortName
tweets.per.min.xts = xts::xts(tweetsPerMinAll[[eventName]][,-which(colnames(tweetsPerMinAll[[eventName]])=="created_at"), drop = FALSE],
    order.by = tweetsPerMinAll[[eventName]][,"created_at"],
    tzone = eventMetadata$scalesTZ)

p = dygraph(tweets.per.min.xts,
    main = paste(eventMetadata$longName, "Tweets passing filter(s) after event", sep = ": "),
    ylab = "Number of tweets per minute") %>%
    dyAxis("x", drawGrid = FALSE,
        label = paste0("Posted time (", eventMetadata$shortTZ, " time zone)"),
        rangePad = 1) %>%
    dyRangeSelector(dateWindow = c(eventMetadata$localTime - minutes(10), eventMetadata$localTime + hours(12))) %>%
    dyHighlight(highlightSeriesOpts = list(strokeWidth = 1.5),
        highlightCircleSize = 3,
        highlightSeriesBackgroundAlpha = 0.8) %>%
    dyEvent(eventMetadata$localTime, "Event", labelLoc = "top") %>% # labelLoc = "bottom"
    dyOptions(includeZero = TRUE, useDataTimezone = TRUE,
        fillGraph = TRUE, fillAlpha = 0.3, colors = "#3B7EA1") %>% # founder's rock
    dyLegend(show = "always", hideOnMouseOut = FALSE)

htmlwidgets::saveWidget(p, "tweetspermin_dygraph3.html")
```

---
class: fullscreen, middle, center

[
```{r tweetspermin_lineplot1, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
i = 1

makeTweetsPerMinLinePlot(tweetsPerMinAll[[twitterEventInfo[i, "shortName"]]],
                         twitterEventInfo[i,])
```
](tweetspermin_dygraph1.html)

```{r dygraph_tweetspermin1, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

i = 1
eventMetadata = twitterEventInfo[i,]
eventName = eventMetadata$shortName
tweets.per.min.xts = xts::xts(tweetsPerMinAll[[eventName]][,-which(colnames(tweetsPerMinAll[[eventName]])=="created_at"), drop = FALSE],
    order.by = tweetsPerMinAll[[eventName]][,"created_at"],
    tzone = eventMetadata$scalesTZ)

p = dygraph(tweets.per.min.xts,
    main = paste(eventMetadata$longName, "Tweets passing filter(s) after event", sep = ": "),
    ylab = "Number of tweets per minute") %>%
    dyAxis("x", drawGrid = FALSE,
        label = paste0("Posted time (", eventMetadata$shortTZ, " time zone)"),
        rangePad = 1) %>%
    dyRangeSelector(dateWindow = c(eventMetadata$localTime - minutes(10), eventMetadata$localTime + hours(12))) %>%
    dyHighlight(highlightSeriesOpts = list(strokeWidth = 1.5),
        highlightCircleSize = 3,
        highlightSeriesBackgroundAlpha = 0.8) %>%
    dyEvent(eventMetadata$localTime, "Event", labelLoc = "top") %>% # labelLoc = "bottom"
    dyOptions(includeZero = TRUE, useDataTimezone = TRUE,
        fillGraph = TRUE, fillAlpha = 0.3, colors = "#3B7EA1") %>% # founder's rock
    dyLegend(show = "always", hideOnMouseOut = FALSE)

htmlwidgets::saveWidget(p, "tweetspermin_dygraph1.html")

# 2:53 p.m. Hospitals receive radio and email notification that a mass casualty incident has occurred at the Finish Line.
# 2:54 p.m. Some area hospitals receive initial estimates of the number of patients to expect.

```



---
class: fullscreen, middle, center

[
```{r tweetspermin_lineplot2, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
i = 2

makeTweetsPerMinLinePlot(tweetsPerMinAll[[twitterEventInfo[i, "shortName"]]],
                         twitterEventInfo[i,])
```
](tweetspermin_dygraph2.html)

```{r dygraph_tweetspermin2, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

i = 2
eventMetadata = twitterEventInfo[i,]
eventName = eventMetadata$shortName
tweets.per.min.xts = xts::xts(tweetsPerMinAll[[eventName]][,-which(colnames(tweetsPerMinAll[[eventName]])=="created_at"), drop = FALSE],
    order.by = tweetsPerMinAll[[eventName]][,"created_at"],
    tzone = eventMetadata$scalesTZ)

p = dygraph(tweets.per.min.xts,
    main = paste(eventMetadata$longName, "Tweets passing filter(s) after event", sep = ": "),
    ylab = "Number of tweets per minute") %>%
    dyAxis("x", drawGrid = FALSE,
        label = paste0("Posted time (", eventMetadata$shortTZ, " time zone)"),
        rangePad = 1) %>%
    dyRangeSelector(dateWindow = c(eventMetadata$localTime - minutes(10), eventMetadata$localTime + hours(12))) %>%
    dyHighlight(highlightSeriesOpts = list(strokeWidth = 1.5),
        highlightCircleSize = 3,
        highlightSeriesBackgroundAlpha = 0.8) %>%
    dyEvent(eventMetadata$localTime, "Event", labelLoc = "top") %>% # labelLoc = "bottom"
    dyOptions(includeZero = TRUE, useDataTimezone = TRUE,
        fillGraph = TRUE, fillAlpha = 0.3, colors = "#3B7EA1") %>% # founder's rock
    dyLegend(show = "always", hideOnMouseOut = FALSE)

htmlwidgets::saveWidget(p, "tweetspermin_dygraph2.html")
```

---
class: fullscreen, middle, center

[
```{r tweetspermin_lineplot4, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
i = 4

makeTweetsPerMinLinePlot(tweetsPerMinAll[[twitterEventInfo[i, "shortName"]]],
                         twitterEventInfo[i,])
```
](tweetspermin_dygraph4.html)

```{r dygraph_tweetspermin4, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

i = 4
eventMetadata = twitterEventInfo[i,]
eventName = eventMetadata$shortName
tweets.per.min.xts = xts::xts(tweetsPerMinAll[[eventName]][,-which(colnames(tweetsPerMinAll[[eventName]])=="created_at"), drop = FALSE],
    order.by = tweetsPerMinAll[[eventName]][,"created_at"],
    tzone = eventMetadata$scalesTZ)

p = dygraph(tweets.per.min.xts,
    main = paste(eventMetadata$longName, "Tweets passing filter(s) after event", sep = ": "),
    ylab = "Number of tweets per minute") %>%
    dyAxis("x", drawGrid = FALSE,
        label = paste0("Posted time (", eventMetadata$shortTZ, " time zone)"),
        rangePad = 1) %>%
    dyRangeSelector(dateWindow = c(eventMetadata$localTime - minutes(10), eventMetadata$localTime + hours(12))) %>%
    dyHighlight(highlightSeriesOpts = list(strokeWidth = 1.5),
        highlightCircleSize = 3,
        highlightSeriesBackgroundAlpha = 0.8) %>%
    dyEvent(eventMetadata$localTime, "Event", labelLoc = "top") %>% # labelLoc = "bottom"
    dyOptions(includeZero = TRUE, useDataTimezone = TRUE,
        fillGraph = TRUE, fillAlpha = 0.3, colors = "#3B7EA1") %>% # founder's rock
    dyLegend(show = "always", hideOnMouseOut = FALSE)

htmlwidgets::saveWidget(p, "tweetspermin_dygraph4.html")
```

???

The tweet graphic signatures were consistent across disasters except SH which had similar signature but, delayed signal initiation.



---

# Signal Threshold

- 200 tweets per minute?
- Initiated before patient arrival to local hospitals*
- Preceded traditional disaster notification and hospital standby notices in SF and NE, and followed within 2 mins in BB & MV

???

A 200 tweets/min threshold was reached fastest with NE (2 min), BB (7 min), and SF (18 mins). If this threshold was utilized as a signaling mechanism to place local hospitals on standby for possible large scale events, in all case studies, this signal would have preceded patient arrival. Importantly, this threshold for signaling would also have preceded traditional disaster notification mechanisms in SF, NE, and simultaneous with BB and MV.

All events reached their overall maximum peak tweet posting per minute rate within 3 hours following the incident with the peak rate being reached fastest in the NE, SF, and MV (Fig 1). The tweet graphic signatures were consistent across disasters except SH which had similar signature but, delayed signal initiation (Fig 1). Peak tweets per minute ranged from 209-3326 for each event with the median in the first 60 minutes from 10-2169 (BB: 2564; SF 549; MV 162; NE 368; SH 10). The total tweets in the first 60 minutes represented a high percentage of the total tweets in the 7 days following the incident time in all cases except SH (11.5% BB, 7.8% SF, 8.0% MV, 11.9% MV, 0.14% SH).

Initial tweets occurred very rapidly in all events with the first signals detected less than 2 mins after the incident start times. When isolating the analysis to only tweets in the first 60 minutes, 1% of the total event specific tweets were reached in a median of 13 minutes of the first 911 calls (Table 3) with the most rapid 1% threshold being reached in the NE (2 minutes), BB (9 minutes), and SF (13 minutes) (Table 3). The maximum tweets per minute in the first 60 minutes was highest for the BB (3326 tweets/minute), followed by SF (1423), MV (957), NE (739) and SH (209) (Fig 2).

Differing thresholds were compared to the county disaster notification time (if a county disaster page was activated), hospital stand-by notification times (if no county disaster page was performed), and patient arrival times (obtained from after action reports). A 200 tweet per minute threshold was reached fastest with NE (2 min), BB (7 min), and SF (18 mins). If this threshold was utilized as a signaling mechanism to place local hospitals on standby for possible large scale events, excluding SH, the signal would have preceded patient arrival to local hospitals in all cases. It would also have preceded traditional county disaster notification and hospital standby notices in SF and NE, and followed within minutes in BB (2.5 minutes).

In both school shootings, a clear county wide disaster activation time was not reported in the after action reports. In the case of MV, there was an all-county wide law enforcement communication sent 28 minutes after the incident. Using this as an indicator in place of an official disaster mass casualty warning, our signal threshold would have been initiated within 2 minutes of this notice and 16 minutes prior to patient arrival at the hospital. For the Sandy Hook (SH) school shooting time line there was no clear time for the county disaster page available or hospital stand by times available in official after action reports making analysis of our signal not possible. There were also no victims transported to hospitals and the first report of casualties for SH did not occur until approximately 40 minutes after the event incident time.

---
class: fullscreen, middle, center

```{r tweetspermin_all_1hr_setup, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
event.tweets.per.min.1hr = vector("list", length(tweetsPerMinAll))
names(event.tweets.per.min.1hr) = names(tweetsPerMinAll)

for(i in names(tweetsPerMinAll)) {
  # event.plus.1hr = interval(subset(twitterEventInfo, shortName == i)$localTime,
                            # subset(twitterEventInfo, shortName == i)$localTime + minutes(60))
  event.tweets.per.min.1hr[[i]] = subset(tweetsPerMinAll[[i]],
                                         created_at < subset(twitterEventInfo, shortName == i)$localTime + minutes(60))
  event.tweets.per.min.1hr[[i]]$event = subset(twitterEventInfo, shortName == i)$longName
  event.tweets.per.min.1hr[[i]]$mins_since_event = difftime(event.tweets.per.min.1hr[[i]]$created_at,
                                lubridate::floor_date(subset(twitterEventInfo, shortName == i)$localTime, unit = "minute"),
                                    units = "mins")
  event.tweets.per.min.1hr[[i]]$cumuln = cumsum(event.tweets.per.min.1hr[[i]]$n)/sum(event.tweets.per.min.1hr[[i]]$n)
}
all.events.tweets.per.min.1hr = do.call(rbind, event.tweets.per.min.1hr)
all.events.tweets.per.min.1hr$event = as.factor(all.events.tweets.per.min.1hr$event)
all.events.tweets.per.min.1hr = all.events.tweets.per.min.1hr[-which(rownames(all.events.tweets.per.min.1hr) == "napa.61"),]

# added 20170217
all.events.tweets.per.min.1hr$event.short = as.character(all.events.tweets.per.min.1hr$event)
all.events.tweets.per.min.1hr[all.events.tweets.per.min.1hr$event=="Boston Marathon bombing", "event.short"] <- "BB"
all.events.tweets.per.min.1hr[all.events.tweets.per.min.1hr$event=="SFO crash", "event.short"] <- "SF"
all.events.tweets.per.min.1hr[all.events.tweets.per.min.1hr$event=="Marysville shooting", "event.short"] <- "MV"
all.events.tweets.per.min.1hr[all.events.tweets.per.min.1hr$event=="Napa earthquake", "event.short"] <- "NE"
all.events.tweets.per.min.1hr[all.events.tweets.per.min.1hr$event=="Sandy Hook shooting", "event.short"] <- "SH"
all.events.tweets.per.min.1hr$event.short = as.factor(all.events.tweets.per.min.1hr$event.short)
```

```{r volume_tweets_1hr_allevents, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
library(directlabels)
ggplot(all.events.tweets.per.min.1hr,
  aes(x = mins_since_event, y = n, color = event)) +
  geom_line(size = 0.75) +
  scale_x_continuous("Minutes Post-Event",
    breaks = seq(from = 0, to = 60, by = 5)) +
  scale_y_continuous("Tweets per minute",
    breaks = seq(from = 0, to = 3500, by = 500),
    limits = c(0, 3500)) +
  ggtitle("Tweet Volume in First 60 Minutes") +
  scale_colour_manual("", values = c("#332288", "#88CCEE", "#117733", "#DDCC77", "#CC6677")) +
  # scale_linetype_manual("", values = c("solid", "dashed", "dotted", "dotdash", "twodash"), guide = "none") +
  theme_twitter_nolines +
    theme(plot.title = element_text(hjust = 0.5)) +
  # theme_classic(base_size = 12) +
  #   theme(legend.position = "bottom",
  #       plot.title = element_text(hjust = 0.5)) +
  geom_dl(aes(label = event.short), method = "last.points") +
  guides(colour = guide_legend(override.aes = list(size=3), nrow = 2))
```

???

Tweets occurred very rapidly for all events (<2 mins)
1% of the total event specific tweets in a median of 13 minutes of the first 911 calls

---
class: fullscreen, middle, center

```{r cumulativepct_tweets_1hr_allevents, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}

ggplot(all.events.tweets.per.min.1hr,
  aes(x = mins_since_event, y = cumuln, color = event)) +
  geom_line(size = 0.75) +
  scale_x_continuous("Minutes Post-Event",
    breaks = seq(from = 0, to = 60, by = 5)) +
  scale_y_continuous("Cumulative percent", labels = scales::percent) +
  ggtitle("Cumulative Tweets in First 60 Minutes") +
  scale_colour_manual("", values = c("#332288", "#88CCEE", "#117733", "#DDCC77", "#CC6677")) +
  theme_twitter_nolines +
    theme(plot.title = element_text(hjust = 0.5)) +
  guides(colour = guide_legend(override.aes = list(size=3), nrow = 2))
```

???

To compare the speed with which tweets accumulated across events, the cumulative tweet volume in the first 60 minutes after each event's onset was also computed.

---

# Hashtag use

Top 20 hashtags for each event, roughly categorized:

* **location**: boston, bostonmarathon, mphs, marysville, newtown, sfo, napa, connecticut, sanfrancisco, sandyhook, california, washington,  watertown, seattle, sf, newton, mit, sanfrancisco, bayarea, americancanyon, ca, sf, napavalley, unitedstates, usa
* **event type**: earthquake, quake, crash, planecrash, manhunt, explosion, terrorist, bomb, shooting, school, schoolshooting
* **event-location hybrids**: ctshooting, sfocrash, napaquake, napaearthquake, sfearthquake, bayareaquake, marysvilleshooting, bostonbombing, mitshooting
* **specific event**: asiana, asianaairlines, asiana214, flight214, boeing777, boeing, plane, ntsb, marathon, jaylenfryberg, wine, 911buff
* **sympathy**: prayfornewtown, prayforboston, prayersforboston, prayersfornewtown, prayforpilchuck, prayfornewton, prayers, sosad, rip
* **news**: breaking, cnn, news, breakingnews, update, retweet, liveonkomo
* **political**: nra, gunfail, gunsense, guncontrol
* **unrelated?**: tcot, p2, ebola

---


```{r sfo_hashtag_streamgraph_setup, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
library(lubridate)
library(dplyr)

eventName = "sfo"
eventMetadata = subset(twitterEventInfo, shortName==eventName)

getHashtagCounts = function(hashtagData) {
    hashtagData$text.lower = as.factor(tolower(hashtagData$text))
    return(as.data.frame(dplyr::count(hashtagData, text.lower, sort = TRUE)))
}
hashtag.counts = sapply(twitter_entities.hashtags.combo, getHashtagCounts, simplify = FALSE)
twitter_entities.hashtags.combo[[eventName]]$text.lower = tolower(twitter_entities.hashtags.combo[[eventName]]$text)

# top20hashtags_allevents = data.frame(boston = hashtag.counts[["boston"]][1:20,]$text.lower,
#                                    marysville = hashtag.counts[["marysville"]][1:20,]$text.lower,
#                                    napa = hashtag.counts[["napa"]][1:20,]$text.lower,
#                                    sandyhook = hashtag.counts[["sandyhook"]][1:20,]$text.lower,
#                                    sfo = hashtag.counts[["sfo"]][1:20,]$text.lower)

twitter_entities.hashtags.combo[[eventName]]$startMinBin = lubridate::floor_date(twitter_entities.hashtags.combo[[eventName]]$postedTime, unit = "minute")
hashtag.counts.bymin = as.data.frame(dplyr::count(twitter_entities.hashtags.combo[[eventName]], startMinBin, text.lower, sort = TRUE))
hashtag.counts.bymin.top20 = subset(hashtag.counts.bymin, text.lower%in%as.character(hashtag.counts[[eventName]][1:20,"text.lower"]))
hashtag.counts.bymin.top20$text.lower = factor(as.character(hashtag.counts.bymin.top20$text.lower),
    levels = as.character(hashtag.counts[[eventName]][1:20,"text.lower"]), ordered = TRUE)

event.allmins = seq(from = trunc(eventMetadata$localTime, units = "mins"),
    # to = trunc(eventMetadata$localTime, units = "mins") + hours(12), by = "min")
    to = round(max(tweetsPerMinAll[[eventName]]$created_at), units = "mins"), by = "min")

hashtag.counts.bymin.top20 = merge(hashtag.counts.bymin.top20,
    expand.grid(startMinBin = event.allmins, text.lower = as.character(hashtag.counts[[eventName]][1:20,"text.lower"])),
    all = TRUE)
hashtag.counts.bymin.top20[is.na(hashtag.counts.bymin.top20$n), "n"] = 0
hashtag.counts.bymin.top20$minnum = (as.numeric(hashtag.counts.bymin.top20$startMinBin) - min(as.numeric(hashtag.counts.bymin.top20$startMinBin)))/60 + 1
```


```{r sfo_hashtag_streamgraph, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
library(streamgraph)
library(viridis)

p = streamgraph(hashtag.counts.bymin.top20[seq(12*60*20), c("text.lower", "n", "minnum")],
        key = "text.lower", value = "n", date = "minnum", scale = "continuous",
        offset = "zero", width = "900", height = "600") %>%
    sg_axis_x(tick_interval = 30) %>%
    sg_fill_manual(viridis(20)) %>%
    sg_legend(show = TRUE, label = "Hashtags: ") #%>%
    # sg_title(title = "Overall top 20 hashtags for SFO crash: use in first 12 hours")

htmlwidgets::saveWidget(p, "sfo_hashtag_streamgraph.html")
```

[![](assets/img/sfo_hashtag_screenshot.png)](sfo_hashtag_streamgraph.html)

---
layout: false
class: inverse, fullscreen, middle, center

# Relevancy

---

layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

---

# Fake News

- \#1 retweet in SF crash Twitter data
    - Retweeted 6,740 times during collection period

![:scale 50%](assets/img/sfo_retweet1.png)

---

# Thoughts and Prayers

- \#2 retweet in SF crash Twitter data
    - Retweeted 3,083 times during collection period

![](assets/img/sfo_retweet2.png)

---

# News

- \#3 and \#4 retweets in SF crash Twitter data
    - Retweeted 1,936 and 1,563 times, respectively, during collection period

![:scale 50%](assets/img/sfo_retweet3.png)

![:scale 50%](assets/img/sfo_retweet4.png)


---

# Eyewitness: 1 min after crash

- \#5 retweet in SF crash Twitter data
    - Retweeted 1,224 times during collection period

![:scale 50%](assets/img/sfo_retweet5.png)

---
layout: false
class: inverse, fullscreen, middle, center

# Localization

---

layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

---

class: fullscreen, middle, center


```{r sfo_leafletplot_setup, message=FALSE, echo=FALSE, cache=TRUE, warning=FALSE}

eventName = "sfo"
tweet.map.df = tweetMapDataAll[[eventName]]

tweet.map.df$lat = tweet.map.df$geo.coordinates.latitude
tweet.map.df[is.na(tweet.map.df$geo.coordinates.latitude), "lat"] = tweet.map.df[is.na(tweet.map.df$geo.coordinates.latitude), "location.geo.coordinate.centroids.latitude"]

tweet.map.df$lng = tweet.map.df$geo.coordinates.longitude
tweet.map.df[is.na(tweet.map.df$geo.coordinates.longitude), "lng"] = tweet.map.df[is.na(tweet.map.df$geo.coordinates.longitude), "location.geo.coordinate.centroids.longitude"]
```

```{r sfo_leafletplot, fig.align='center', out.width='100%', out.height='100%', message=FALSE, echo=FALSE, cache=TRUE, warning=FALSE}
library(leaflet)

# "object.geo.coordinates.latitude", "object.geo.coordinates.longitude"
# "object.location.geo.coordinate.centroids.latitude", "object.location.geo.coordinate.centroids.longitude"

p = leaflet(tweet.map.df[,c("lat", "lng", "body", "postedTime", "object.link")]) %>% #addTiles() %>%
  addProviderTiles(providers$OpenStreetMap.Mapnik) %>%
  addMarkers(~lng, ~lat, popup = ~paste0(postedTime, ": ", body, " (", object.link, ")"),
    clusterOptions = markerClusterOptions())

htmlwidgets::saveWidget(p, "sfo_leaflet.html")
```

[![](assets/img/sfo_leaflet_screenshot.png)](sfo_leaflet.html)

```{r sfo_ggmap, message=FALSE, echo=FALSE, eval=FALSE, warning=FALSE}
library(ggmap)

USAMap +
    geom_point(aes(x=lon, y=lat), data=mv_num_collisions, col="orange", alpha=0.4, size=mv_num_collisions$collisions*circle_scale_amt) +
    scale_size_continuous(range=range(mv_num_collisions$collisions))

```

---
layout: false
class: inverse, fullscreen, middle, center

# Data Visualization

---

layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

---

# Visualization

**[ggplot2](http://ggplot2.tidyverse.org)**
- [package source](https://github.com/tidyverse/ggplot2), [docs](http://ggplot2.tidyverse.org/reference/)

.pull-left[

**[htmlwidgets](http://www.htmlwidgets.org/)**: interactive data visualization framework in R
- provides the framework on which most of the interactive plotting packages in R rely (if they use JavaScript plotting libraries on the backend).
- [package source](https://github.com/ramnathv/htmlwidgets), [docs](http://www.htmlwidgets.org/), [gallery](http://gallery.htmlwidgets.org/)
- Advanced usage may require knowledge/use of JavaScript.
- Use with [R Markdown](http://rmarkdown.rstudio.com) or [Shiny](http://shiny.rstudio.com/)
]

.pull-right[
- [sparkline](https://github.com/htmlwidgets/sparkline): interactive versions of very small/inline graphics (Edward Tufte)
- [DT](https://github.com/rstudio/DT) ([docs](http://rstudio.github.io/DT/)): interactive tables (aka DataTables)
- [dygraphs](https://github.com/rstudio/dygraphs) ([docs](http://rstudio.github.io/dygraphs)): interactive time series plots
- [streamgraph](http://hrbrmstr.github.io/streamgraph/): interactive streamgraphs, also for time series
- [leaflet](https://github.com/rstudio/leaflet) ([docs](http://rstudio.github.io/leaflet)): interactive maps
* $\ldots$
]

---

# More `htmlwidgets` packages

.pull-left[
- [Highcharter](http://jkunst.com/highcharter/): interface to the Highcharts JavaScript library
- [formattable](https://renkun.me/formattable/): visualize/format vectors and data frames
- [threejs](https://github.com/bwlewis/rthreejs): interactive 3d plots and globes
- [MetricsGraphics](http://hrbrmstr.github.io/metricsgraphics/): interface to the MetricsGraphics D3 JavaScript library
- [rbokeh](https://github.com/bokeh/rbokeh) ([docs](http://hafen.github.io/rbokeh/), [useR2016 slides](http://slides.com/hafen/rbokeh#/)): interface to the Bokeh Python library
- [networkD3](http://christophergandrud.github.io/networkD3/): interactive [D3](http://d3js.org/) JavaScript network graphs
- [visNetwork](https://github.com/datastorm-open/visNetwork) ([docs](http://datastorm-open.github.io/visNetwork)): interactive network graphs
]

.pull-right[
- [plotly](https://github.com/ropensci/plotly) ([docs](https://plot.ly/r/)):
    - Convert `ggplot2` graphics $\rightarrow$ interactive graphics via `ggplotly()` and the plot.ly [ggplot2 library](https://plot.ly/ggplot2/) OR
    - Create interactive graphics directly with `plot_ly()` and the plot.ly [R library](https://plot.ly/r/)
    - Can operate entirely locally (your plot isn't automatically uploaded to plot.ly's servers and shared with the world).
- [ggiraph](https://github.com/davidgohel/ggiraph); [ggiraphExtra](https://github.com/cardiomoon/ggiraphExtra): make `ggplot2` graphics interactive.
- [d3heatmap](https://github.com/rstudio/d3heatmap) ([demo](http://rpubs.com/jcheng/mtcars-heatmap)): interactive heatmaps
]

---
layout: false
class: inverse, fullscreen, middle, center

# Comparison with Prospective Twitter data

---

layout: true

<div class="my-footer"><span><a href="http://www.saraemoore.com/TwitterTraumaBBD2017">github:saraemoore/TwitterTraumaBBD2017</a></span></div>

---

```{r superbowl_metadata, message=FALSE, echo=FALSE, warning=FALSE}
trackTermsLocation = c("sf", "sanfrancisco", "san", "francisco", "sanfran",
  "prayforsanfrancisco", "bayarea", "sfbayarea", "bay", "sanfranciscobay",
  "northerncalifornia", "norcal", "california", "ca",
  "sfgh", "zfgh", "thegeneral", "general", "goldengatebridge", "goldengate",
  "bridge", "baybridge", "bart", "caltrain", "muni", "cablecar")
trackTermsEvent = c("breaking", "news", "breakingnews", "cnn", "pray",
  "crash", "shot", "shooting", "stab", "stabbed", "stabbing", "fall",
  "dead", "died", "accident", "earthquake", "flood", "victim", "victims",
  "fatality", "fatalities", "attack")
```

```{r loaddata_superbowl, message=FALSE, echo=FALSE, warning=FALSE, cache=TRUE}
tweet_hist_data = readRDS(file.path(baseDir, "tweet_hist_data.rds"))
```

# Super Bowl 50

.pull-left[
- Match 1+ location keywords AND/OR 1+ event keywords
- 2 weeks (Sun, 31 Jan 2016 until Sun, 14 Feb 2016)
- Collected `r prettyNum(sum(tweet_hist_data[["unfilt"]]$n), big.mark = ",")` tweets, retweets, and quote tweets
    - +248 truncated records (discarded)
    - +6,675,076 records not captured (exceeded rate limit)
]

.pull-right[
.smaller[
- **'Event' keywords:** `r paste(trackTermsEvent, collapse = ", ")`
- **'Location' keywords:** `r paste(trackTermsLocation, collapse = ", ")`
]
]

???

Event & location key words utilized in the prospective test signal for Super Bowl 50.

A prospective API (R-based application program interface) was then constructed using generic search terms for potential multiple casualty events and was prospectively used to gather data from Twitter via their API (application programming interface) during a recent high profile sporting event (SF Super Bowl 50). SB50 was to be used as a control event to test the tweet per minute thresholds developed from the prior 5 US multiple casualty events. We compared the multiple casualty event results to the control event to determine how often a warning would have fired in error for various tweet/minute threshold signals using potential multiple casualty search terms or 'event' and a 'location' keyword. The key words were developed by studying both the most frequently used hashtags and keywords common across the five previously analyzed events ("breakingnews", "cnn", "pray", etc.) and by utilizing knowledge of locally relevant hashtags and keywords ("sf", "sfgh", "zfgh", "thegeneral", etc.).

From Sun, Jan 31, 2016 at 18:43:14 PST until Sun, Feb 14, 2016 at 16:03:49 PST, connected to Twitter's 'POST statuses/filter' public stream with the following 'track keywords'/'filter predicates', where a 'phrase' matched if *any* of the terms were present in the Tweet (order- and case-insensitive):

According to Twitter, "The text of the Tweet and some entity fields are considered for matches. Specifically, the `text` attribute of the Tweet, `expanded_url` and `display_url` for links and media, `text` for hashtags, and `screen_name` for user mentions are checked for matches."

Moving Average calculated

<!--
- No signal activation even during the height of the Super Bowl activities
-->

As a proof of concept exercise, a prospective API using generic search terms for the event was built in anticipation of the Super Bowl 50 to test the threshold of 200 tweets per minute for triggering notification of multiple casualty events. The goal was to test whether a high profile, frequently tweeted public event would falsely trigger the signal even if no mass casualty situation arose during the event. The API gathered relevant tweets that included one or more of the generic terms plus a location or event term for a two week period beginning one week before the Super Bowl and continuing for 7 days following the Super Bowl.

An estimated 55,355,870 original tweets, retweets, and quote tweets were collected over the approximately two week period. Not included in this total were 248 partial or truncated tweets which were discarded for incomplete data. The contents of an estimated 6,675,076 additional tweets were not captured because, at some moments, the provided keywords matched more tweets than Twitter's imposed rate limit allowed to be delivered.


Estimated total number of tweets (including retweets and quote tweets) collected: `r prettyNum(sum(tweet_hist_data[["unfilt"]]$n), big.mark = ",")` <!--55,355,870-->. This estimate takes into account the 248 partial/truncated tweets collected but discarded for incomplete data as well as an estimated 6,675,076 records which were not captured because the keywords matched more Tweets than Twitter's imposed rate limit allowed to be delivered.

The volume of Super Bowl tweets returned at any moment in time were subject to a "streaming cap": the free public filtered streams offered by Twitter "max out" at a small percentage (approximately 1%) of the total volume of tweets coming through Twitter's "firehose." When the streaming cap is exceeded, a rate limiting message is issued by the streaming service indicating how many tweets were missed. Tweets were collected into JSON files which were rotated approximately every half hour. JSON files were preprocessed via shell script and imported and cleaned utilizing the jsonlite and tidyjson packages in R. Tweets containing at least one event-related keyword AND at least one location-related keyword were summarized as tweets per minute over the entire collection period.

---
class: fullscreen, middle, center

```{r sb_kickoff_tweets_per_min_setup, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
# super bowl: sun, feb 7 at 6:30pm ET

sb.kickoff.time = lubridate::ymd_hm("2016 Feb 7 15:30", tz = "America/Los_Angeles")
sb.kickoff.12hrs = interval(sb.kickoff.time - hours(12), sb.kickoff.time + hours(12))
```

[
```{r dygraphtweetsperminplot1, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# Summary of tweets passing event OR location keyword filter in a two hour period, by minute
tweet_hist_data[["unfilt"]] = plyr::rename(tweet_hist_data[["unfilt"]], c("n" = "count"))
tweets.per.min.unfilt.xts = xts::xts(
    tweet_hist_data[["unfilt"]][,-which(colnames(tweet_hist_data[["unfilt"]])=="created_at"), drop = FALSE],
    order.by = tweet_hist_data[["unfilt"]][,"created_at"],
    tzone = "America/Los_Angeles")

p = dygraph(tweets.per.min.unfilt.xts,
    main = "Histogram of tweets passing event OR location keyword filter",
    ylab = "Number of tweets per minute") %>%
    dyAxis("x", drawGrid = FALSE,
        label = "Posted time (local time zone)", 
        rangePad = 1) %>%
    dyRangeSelector() %>%
    dyHighlight(highlightSeriesOpts = list(strokeWidth = 1.5),
        highlightCircleSize = 3, 
        highlightSeriesBackgroundAlpha = 0.8) %>%
    # dyEvent(strptime("2016-02-02 10:05:00-0700", "%Y-%m-%d %H:%M:%S%z", tz = "UTC"), "Event range start", labelLoc = "top") %>%
    # dyEvent(parse_date_time("Feb 2 2016 10:05", "b d Y H:M", tz = "America/Los_Angeles"), "Event range", labelLoc = "bottom") %>%
    # dyEvent(parse_date_time("Feb 2 2016 10:10", "b d Y H:M", tz = "America/Los_Angeles"), "Event range", labelLoc = "bottom") %>%
    # dyShading(parse_date_time("Feb 2 2016 10:05", "b d Y H:M", tz = "America/Los_Angeles"),
        # parse_date_time("Feb 2 2016 10:10", "b d Y H:M", tz = "America/Los_Angeles"),
        # color = "#EFEFEF", axis = "x") %>%
    dyEvent(parse_date_time("Feb 7 2016 15:30", "b d Y H:M", tz = "America/Los_Angeles"), "Super Bowl kickoff", labelLoc = "bottom") %>%
    dyOptions(includeZero = TRUE, useDataTimezone = TRUE, fillGraph = TRUE, fillAlpha = 0.3) %>%
    dyLegend(show = "always", hideOnMouseOut = FALSE)

htmlwidgets::saveWidget(widget = p, file="sb_tweets_per_min_unfiltered.html")

colnames(tweet_hist_data[["unfilt"]])[2] = "n"
makeTwitterTSPlot(tweet_hist_data[["unfilt"]],
     sb.kickoff.12hrs,
     breaks.x = "24 hours",
     fill.under.line = FALSE,
     plot.subtitle = "",
         event.label = "Super Bowl kickoff occurred at\napprox. 3:30pm on 7 Feb",
         event.label.y = 1000, event.time = sb.kickoff.time)
```
](sb_tweets_per_min_unfiltered.html)

---
class: fullscreen, middle, center

```{r sb_kickoff_12hrs_tweets_per_min_filtered_userpactime, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% sb.kickoff.12hrs),
     sb.kickoff.12hrs,
         line.colors = "gray40",
         event.label = "Super Bowl kickoff occurred at\napprox. 3:30pm on 7 Feb",
         event.label.y = 300, event.time = sb.kickoff.time)
```

```{r sb_kickoff_12hrs_tweets_per_min_filtered_userpactime_line, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% sb.kickoff.12hrs),
     sb.kickoff.12hrs,
     fill.under.line = FALSE,
     event.label = "Super Bowl kickoff occurred at\napprox. 3:30pm on 7 Feb",
     event.label.y = 300, event.time = sb.kickoff.time)
```

---

# Suspicious spike


.pull-left[
[![:scale 90%](assets/img/tweetstorm_screenshot_20161219.jpg)](https://twitter.com/Centric510/status/696556613109329920)
]

.pull-right[
- Many retweets of a single tweet in rapid succession
    - 419 times in 29 seconds
    - Auto-retweets/bots?
- Lesson: ignore unsustained peaks
    - $\rightarrow$ smooth the time series
]

???

Screenshot captured on 19 Dec 2016

419 times were between 7 Feb 2016 9:05:30pm PST and 7 Feb 2016 9:05:59pm PST

---
class: fullscreen, middle, center

```{r sb_MA_setup, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
library(zoo)

midweek.time = lubridate::ymd_hm("2016 Feb 5 12:00", tz = "America/Los_Angeles")
midweek.12hrs = interval(midweek.time - hours(12), midweek.time + hours(12))

tweet_hist_data.bothfilt.userpactime.copy <- tweet_hist_data[["bothfilt.userpactime"]]
bothfilt.userpactime.zoo = zoo(tweet_hist_data.bothfilt.userpactime.copy$n, tweet_hist_data.bothfilt.userpactime.copy$created_at)

movavg.5mins = rollmean(bothfilt.userpactime.zoo, 5, fill = list(NA, NULL, NA))
movavg.60mins = rollmean(bothfilt.userpactime.zoo, 60, fill = list(NA, NULL, NA))

tweet_hist_data.bothfilt.userpactime.copy$n = coredata(movavg.5mins)
tweet_hist_data.bothfilt.userpactime_5minMA = rbind(tweet_hist_data[["bothfilt.userpactime"]], tweet_hist_data.bothfilt.userpactime.copy)
tweet_hist_data.bothfilt.userpactime_5minMA$count.type = as.factor(rep(c("Actual", "Moving Average (5 min window)"),
                                                              each = nrow(tweet_hist_data.bothfilt.userpactime.copy)))

tweet_hist_data.bothfilt.userpactime.copy$n = coredata(movavg.60mins)
tweet_hist_data.bothfilt.userpactime_60minMA = rbind(tweet_hist_data[["bothfilt.userpactime"]], tweet_hist_data.bothfilt.userpactime.copy)
tweet_hist_data.bothfilt.userpactime_60minMA$count.type = as.factor(rep(c("Actual", "Moving Average (60 min window)"),
                                                              each = nrow(tweet_hist_data.bothfilt.userpactime.copy)))
```

```{r sb_kickoff_12hrs_tweets_per_min_filtered_userpactime_line_withMA, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data.bothfilt.userpactime_5minMA, created_at %within% sb.kickoff.12hrs),
     sb.kickoff.12hrs,
     fill.under.line = FALSE,
     multi.line = TRUE,
     event.label = "Super Bowl kickoff occurred at\napprox. 3:30pm on 7 Feb",
     event.label.y = 300, event.time = sb.kickoff.time)
```

```{r sb_kickoff_12hrs_tweets_per_min_filtered_userpactime_line_withMA_notitle, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
# added 20170217
makeTwitterTSPlot(subset(tweet_hist_data.bothfilt.userpactime_5minMA, created_at %within% sb.kickoff.12hrs),
     sb.kickoff.12hrs,
     fill.under.line = FALSE,
     multi.line = TRUE,
     plot.title = NULL,
     plot.subtitle = NULL,
     event.label = "Super Bowl kickoff occurred at\napprox. 3:30pm on 7 Feb",
     event.label.y = 300, event.time = sb.kickoff.time)
```

```{r bkgdnoise_5feb_tweets_per_min_filtered_userpactime_line_with60minMA, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
# more smoothing to roughly quantify "background noise" on a day when no big events happened (Fri Feb 5)
makeTwitterTSPlot(subset(tweet_hist_data.bothfilt.userpactime_60minMA, created_at %within% midweek.12hrs),
     midweek.12hrs,
     fill.under.line = FALSE,
     multi.line = TRUE,
     include.max.annot = FALSE,
     event.label.y = 300,
     line.colors = c("gray20", "#e08214"))
```

---

# Unexpected Low Casualty Event

![](assets/img/chp_stabbing_sfgate.png)

???

http://www.sfgate.com/crime/article/CHP-Officer-attacked-and-injured-in-San-Francisco-6801249.php

Although not planned, while the streaming data collection was running, a high profile but low casualty count (one victim) event occurred during the Super Bowl near the Fan Zone Experience in San Francisco. This event provided an additional test to our signal to determine if a low casualty count but high profile trauma would falsely activate our signal. On Tuesday, February 2, 2016, between approximately 10:05 (time encounter began) and 10:10 (time of distress call) PST, a California Highway Patrol (CHP) officer was stabbed in San Francisco [16]. The first tweet about the stabbing from an SFPD officer appeared at 10:30 PST [17].

---
class: fullscreen, middle, center

```{r setupchpplots, message=FALSE, echo=FALSE, warning=FALSE}
chp.event.times = lubridate::ymd_hm(c("2016 Feb 2 10:05", "2016 Feb 2 10:10"), tz = "America/Los_Angeles")
chp.event.12hrs = interval(min(chp.event.times) - hours(12), max(chp.event.times) + hours(12))
chp.event.2hrs = interval(min(chp.event.times) - minutes(5), max(chp.event.times) + minutes(110))
# chp.10am.to.10am = interval(min(chp.event.times) - minutes(5) - hours(12),
#                           min(chp.event.times) - minutes(5) + hours(12))
```


```{r chp_12hrs_tweets_per_min_filtered_userpactime, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% chp.event.12hrs),
     chp.event.12hrs,
         event.label = "CHP stabbing occurred between\n10:05am and 10:10am on 2 Feb",
         event.time = chp.event.times,
         line.colors = "gray40",
         adjust.y.max = 1.1, event.label.y = 122, event.label.size = 3.25)
```

```{r chp_12hrs_tweets_per_min_filtered_userpactime_line, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% chp.event.12hrs),
     chp.event.12hrs,
     fill.under.line = FALSE,
     event.label = "CHP stabbing occurred between\n10:05am and 10:10am on 2 Feb",
     event.time = chp.event.times,
         adjust.y.max = 1.1, event.label.y = 122, event.label.size = 3.25)
```

```{r chp_12hrs_tweets_per_min_filtered_userpactime_line_notitle, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
# added 20170217
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% chp.event.12hrs),
     chp.event.12hrs,
     fill.under.line = FALSE,
     event.label = "CHP stabbing occurred between\n10:05am and 10:10am on 2 Feb",
     plot.title = NULL,
     plot.subtitle = NULL,
     event.time = chp.event.times,
         adjust.y.max = 1.1, event.label.y = 122, event.label.size = 3.25)
```

```{r chp_12hrs_tweets_per_min_filtered_userpactime_line_horizlabel, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% chp.event.12hrs),
     chp.event.12hrs, breaks.x = "2 hours",
     fill.under.line = FALSE, event.line.alpha = 0.6,
     subtitle.color = "#FF6666", subtitle.size = rel(0.8),
     plot.subtitle = "CHP stabbing occurred between 10:05am and 10:10am on 2 Feb",
     plot.title = "Summary of tweets passing event AND location keyword filters\nand where users self-reported Pacific time zone",
     event.time = chp.event.times)
```

class: fullscreen, middle, center

```{r chp_12hrs_tweets_per_min_filtered_userpactime_line_withMA, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
makeTwitterTSPlot(subset(tweet_hist_data.bothfilt.userpactime_5minMA, created_at %within% chp.event.12hrs),
     chp.event.12hrs,
     fill.under.line = FALSE,
     multi.line = TRUE,
     event.label = "CHP stabbing occurred between\n10:05am and 10:10am on 2 Feb",
     event.time = chp.event.times,
         adjust.y.max = 1.1, event.label.y = 122, event.label.size = 2.5)
```

class: fullscreen, middle, center

```{r chp_2hrs_tweets_per_min_filtered_userpactime, message=FALSE, echo=FALSE, cache=FALSE, warning=FALSE, eval=FALSE}
# added 20170207
makeTwitterTSPlot(subset(tweet_hist_data[["bothfilt.userpactime"]], created_at %within% chp.event.2hrs),
     chp.event.2hrs, breaks.x = "15 mins",
     fill.under.line = FALSE, event.line.alpha = 0.6,
     subtitle.color = "#FF6666", subtitle.size = rel(0.8),
     plot.subtitle = "CHP stabbing occurred between 10:05am and 10:10am on 2 Feb",
     label.format.x = "%l:%M%P",
     plot.title = "Summary of tweets passing event AND location keyword filters\nand where users self-reported Pacific time zone",
     event.time = chp.event.times)
```

???


No signal activation for a High Profile, but low casualty event

Between 10:00 and 12:00 on this date, 343,084 tweets were collected (Fig 3). After filtering the text of tweets for those which contained at least one event-related keyword and at least one location-related keyword in the text of the tweet, the total number of tweets over the two hour period was reduced to 72,952. In the two hours following this event, a signal using our key generic words with restriction to location set at the 200 tweet per minute threshold would not have fired (Fig 3) indicating that use of this as a large scale multiple casualty signal for placing hospitals on standby would have worked correctly during a high profile, low casualty event. In addition, the signal would not have been activated even during the height of the Super Bowl activities (during game time) again supported it as a potential signaling threshold for early warning detection (Fig 4).

<!--
Plots below look in depth at tweets on Tue, Feb 2, 2016 between 10am and noon PST, when a CHP officer was stabbed in San Francisco between 10:05 and 10:10am (approximately). SFPD's first tweet about the stabbing appeared at 10:30am. 343,084 tweets were collected during this interval (10am to noon). After filtering for tweets which contain *both* a location and an event keyword in the text of the tweet (or original tweet, if the record is a retweet or quoted tweet), the number of records drops to 72,952. Filtering on only event keywords results in the retention of 192,251 records.
-->

On Tue, Feb 2, 2016, between 10:05 and 10:10am PST (approximately), a CHP officer was stabbed in San Francisco. SFPD's first tweet about the stabbing appeared at 10:30am.



---

# Conclusions

- Rapid use and scalability
- Geographic term included
- Proof of Concept Study
- Powerful, predictable, and potentially important resource for optimizing disaster response
- Leverage advanced analytics for prospective monitoring

???

Social media platforms including Twitter have great potential for use in formalized disaster planning and response. Social media data has demonstrated that this mechanism is a powerful, predictable, and potentially important resource for optimizing disaster response. Further investigation is warranted to assess the utility of prospective signal thresholds for hospital based activation.

Our findings confirm that information flows via Twitter extremely rapidly after an event of interest. In the 5 case events examined, the first relevant tweets appeared in under 2 minutes from the estimated start time of each event in the after action reports. These tweets were posted prior to 911 calls in all the events except for the Sandy Hook (SH) school shooting. Fig 1 demonstrates the rapid use and scalability and underscores the potential power of this modality for information dissemination. We believe that SH may have had a delayed scalability in the first hour following the event because this event took place at an elementary school where the number of persons on site during the time of the initial event with access to a device allowing posting to social media would have been extremely limited. In addition, the tweet signature is also likely a reflection of the number of secondary witnesses or the public nature of the disaster taking place. For those events occurring in highly populated areas or events, the number of potential witnesses and Twitter users will be greater than in events occurring in geographically isolated areas or low populated events.

This proof of concept study has shown that a non-traditional data source like Twitter could be utilized to develop early warning signals of multiple casualty events that require hospital based responses that exceed normal operations. A 200 tweet per minute threshold signal in this study would have resulted in hospital notification before patient arrival in all cases where injuries requiring hospital based care were present. In addition, it would have sent notification prior to or within 3 minutes of county disaster signals in cases where county wide alerts were activated. Simultaneous or nearly simultaneous notification is still valuable given that redundant communications are needed given the high rate of failure of infrastructure that often occurs following many mass casualty events.

Although more work is needed to identify the optimum tweet per minute threshold that would result in an appropriate sensitivity and specificity for a wide spectrum of hospital based disaster activations, a signal developed from generic trauma search terms coupled with terms that are geographically restricted has real promise. To be generalizable, our data utilized generic terms such as 'crash, accident, shot, shooting, stab, stabbed, stabbing, fall, dead, died, earthquake, flood, hurricane, tornado, victim, victims, fatality, fatalities, attack' coupled with a geographic restriction. However, to reflect the evolution of multiple casualty events since the original 5 included in this study occurred, we suggest expanding these generic terms to include 'terrorist' and 'terror.' It will be necessary if these signals are incorporated into practice, to continue to iterate on the most relevant terms as the safety and security of our world evolves.

While promising in concept, the data presented in this paper can not be used to determine the false negative rate of a signal applied across a spectrum of disaster scenarios. Disasters remain unpredictable in frequency making prospective testing of the signal challenging both from a data storage standpoint and practicality standpoint. These signals will be most helpful if they are geographically restricted, but predicting in advance that an event will definitely occur to allow testing of the adapted signal for a given region is nearly impossible. One could create a signal for a given location and no event ever takes place. In contrast, it is impractical without massive compute infrastructure to prospectively monitor all geographic areas in the world for the next large scale disaster. Thus, the determination of the false negative rate (failure to activate for a real event) will remain reliant on retrospective tweet data ascertained following future disaster events of varying types.

Although there are no standard approaches adopted widespread for the use of social media platforms for disaster planning and notification, there are many potential advantages to social media over traditional resources for communication of initial information in disaster times. Social media is inexpensive, rapidly disseminated, allows 2 way communication, scalable through 'retweets,' and not dependent upon a traditional power grid. In fact, there have been several worldwide incidences that have cited Twitter as the only source of communication available when other traditional communications have failed. In contrast, traditional media sources filter the message, are only one-way communication, have a slower dissemination, lack the same scalability, are dependent upon the power grid, and exclude geographically remote areas. Further, standard communication systems have failed in a number of recent events including the Taiwan typhoon in 2009, 2010 Haiti earthquake, and alternative communication sources including Twitter were relied upon heavily in the 2011 Egyptian uprising. Merchant et al. note that social media has great potential to increase 'our ability to prepare for, respond to, and recover from events that threaten the public's health.'

These social media tools coupled with advanced analytics have great promise for further development of prospective early warning disaster signals, but they also present a number of limitations. To be most effective, the signals need to be geographically specific and thus, would require some customization. In contrast, the search terms the signals are built from as demonstrated by our Super Bowl test of the signal can be useful even if they are quite generic. Events that occur in more remote geographic areas that lack access to internet would be less likely to be sensitive to a signal built on social media activity. Further, there is differential use of social media across differing age groups, socioeconomic statuses, and geographically. Finally, infrastructure disruption in wide spread disasters may limit the use of social media and responding agencies must have access to social media for this tool to be effective. Each of these could limit the applicability of these types of warnings.


---

# Future work

* Retrospective collection of Twitter data on MCI(s) using refined/generic search terms
    - Las Vegas?
* NLP/sentiment analysis of tweet text
* Real-time 'dashboard'

---

# Further reading

<!--
<sup>1</sup>
-->

* `r TextCite(bib, "twittertrauma2017")`, "Finding the Signal in the Noise: Could Social Media Be Utilized for Early Hospital Notification of Multiple Casualty Events?"
* `r TextCite(bib, "asianacrash2016")`, "Reconsidering the resources needed for multiple casualty events: Lessons learned from the crash of Asiana airlines flight 214"

<!--
.footnote[1: `r TextCite(bib, "twittertrauma2017")`]
-->

---
layout: false
class: inverse, fullscreen, middle, center

# Questions?

---

# References

.small[

```{r printbib, results="asis", echo=FALSE, cache=FALSE}
PrintBibliography(bib, .opts = list(check.entries = FALSE, sorting = "ynt", no.print.fields = c("doi")))
```

]

---

# Credits

#### Built using [xaringan](https://github.com/yihui/xaringan) with
+ [knitr](http://yihui.name/knitr),
+ [R Markdown](https://rmarkdown.rstudio.com),
+ the [remark.js](https://github.com/gnab/remark/) framework,
+ the [RefManageR](https://github.com/ropensci/RefManageR/) bibliography manager, and
+ the [highlight.js](https://highlightjs.org/) syntax highlighter.

---

# Session Info

```{r sessionInfo, results="markup", echo=FALSE, message=FALSE, warning=FALSE}
library(pander)
pander(sessionInfo(), locale = FALSE, compact = TRUE)
```